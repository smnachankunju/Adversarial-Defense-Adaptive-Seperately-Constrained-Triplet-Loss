{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchattacks","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T01:35:15.499996Z","iopub.execute_input":"2025-12-18T01:35:15.500315Z","iopub.status.idle":"2025-12-18T01:35:18.935663Z","shell.execute_reply.started":"2025-12-18T01:35:15.500282Z","shell.execute_reply":"2025-12-18T01:35:18.934699Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchattacks in /usr/local/lib/python3.11/dist-packages (3.5.1)\nRequirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (0.21.0+cu124)\nRequirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (1.15.3)\nRequirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (4.67.1)\nRequirement already satisfied: requests~=2.25.1 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (2.25.1)\nRequirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (2.4.1)\nRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.25.1->torchattacks) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.25.1->torchattacks) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.25.1->torchattacks) (1.26.20)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.25.1->torchattacks) (2025.10.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->torchattacks) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.8.2->torchattacks) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.4->torchattacks) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.4->torchattacks) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.4->torchattacks) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.4->torchattacks) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.4->torchattacks) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.4->torchattacks) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom collections import defaultdict\nimport random\nimport time\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Available GPUs: {torch.cuda.device_count()}\")\n\n\n# ==================== ResNet Building Blocks ====================\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                               stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels * self.expansion:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels * self.expansion,\n                         kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * self.expansion)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNetEmbedding(nn.Module):\n    def __init__(self, block, num_blocks, embedding_dim=128, dropout=0.1):\n        super(ResNetEmbedding, self).__init__()\n        self.in_channels = 64\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.dropout = nn.Dropout2d(dropout)\n        \n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, embedding_dim)\n        self.bn_emb = nn.BatchNorm1d(embedding_dim)\n        \n    def _make_layer(self, block, out_channels, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels * block.expansion\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.dropout(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        out = self.bn_emb(out)\n        return F.normalize(out, p=2, dim=1)\n\n\ndef ResNet18Embedding(embedding_dim=128, dropout=0.1):\n    return ResNetEmbedding(BasicBlock, [2, 2, 2, 2], embedding_dim, dropout)\n\n\nclass TripletNetwork(nn.Module):\n    def __init__(self, embedding_net):\n        super(TripletNetwork, self).__init__()\n        self.embedding_net = embedding_net\n        \n    def forward(self, anchor, positive, negative):\n        anchor_embedding = self.embedding_net(anchor)\n        positive_embedding = self.embedding_net(positive)\n        negative_embedding = self.embedding_net(negative)\n        return anchor_embedding, positive_embedding, negative_embedding\n    \n    def get_embedding(self, x):\n        return self.embedding_net(x)\n\n\ndef get_embedding_safe(model, x):\n    \"\"\"Safely get embedding from model (handles DataParallel)\"\"\"\n    if isinstance(model, nn.DataParallel):\n        return model.module.get_embedding(x)\n    else:\n        return model.get_embedding(x)\n\n\n# ==================== Optimized ART-AL Loss ====================\nclass ARTALLoss(nn.Module):\n    def __init__(self, m1=0.5, m2=2.0, lambda_min=0.2, lambda_max=0.8):\n        super(ARTALLoss, self).__init__()\n        self.m1 = m1\n        self.m2 = m2\n        self.lambda_min = lambda_min\n        self.lambda_max = lambda_max\n        self.lambda_k = 0.5\n        self.gamma_k = 0.0\n        \n    def forward(self, anchor, positive, negative, anchor_adv=None, positive_adv=None):\n        pos_dist = torch.sum((anchor - positive) ** 2, dim=1)\n        neg_dist = torch.sum((anchor - negative) ** 2, dim=1)\n        \n        pos_dist = torch.clamp(pos_dist, min=0.0, max=4.0)\n        neg_dist = torch.clamp(neg_dist, min=0.0, max=4.0)\n        \n        intra_loss = torch.clamp(pos_dist - self.m1, min=0.0)\n        inter_loss = torch.clamp(self.m2 - neg_dist, min=0.0)\n        \n        intra_loss_mean = torch.mean(intra_loss)\n        inter_loss_mean = torch.mean(inter_loss)\n        \n        if anchor_adv is not None and positive_adv is not None:\n            adv_anchor_dist = torch.sum((anchor - anchor_adv) ** 2, dim=1)\n            adv_positive_dist = torch.sum((positive - positive_adv) ** 2, dim=1)\n            adv_loss = torch.mean(torch.clamp(adv_anchor_dist + adv_positive_dist, max=2.0))\n            self.gamma_k = min(0.3, self.gamma_k + 0.015)\n        else:\n            adv_loss = torch.tensor(0.0).to(anchor.device)\n            self.gamma_k = 0.0\n        \n        total_base = intra_loss_mean.item() + inter_loss_mean.item()\n        if total_base > 1e-6:\n            new_lambda = intra_loss_mean.item() / (total_base + 1e-8)\n            self.lambda_k = 0.8 * self.lambda_k + 0.2 * new_lambda\n            self.lambda_k = max(self.lambda_min, min(self.lambda_max, self.lambda_k))\n        \n        base_weight = 1.0 - self.gamma_k\n        loss = base_weight * ((1 - self.lambda_k) * intra_loss_mean + \n                             self.lambda_k * inter_loss_mean) + self.gamma_k * adv_loss\n        \n        return loss, intra_loss_mean, inter_loss_mean, adv_loss, self.lambda_k, self.gamma_k\n\n\n# ==================== FIXED PGD Attack (Works in both train and eval) ====================\ndef generate_pgd_attack(model, images, epsilon=8/255, alpha=2/255, num_iter=5):\n    \"\"\"\n    PGD attack that works correctly in both training and evaluation contexts\n    \"\"\"\n    # Store original training state\n    was_training = model.training\n    model.eval()  # Always use eval mode for PGD attack\n    \n    # Clone images and enable gradients\n    images = images.detach().clone()\n    images.requires_grad = True\n    \n    # Initialize perturbation\n    delta = torch.zeros_like(images).uniform_(-epsilon, epsilon)\n    delta.requires_grad = True\n    \n    for _ in range(num_iter):\n        # Forward pass with perturbation\n        perturbed = images + delta\n        embeddings = get_embedding_safe(model, perturbed)\n        \n        # Loss: maximize embedding magnitude (simple adversarial objective)\n        loss = -torch.mean(torch.sum(embeddings ** 2, dim=1))\n        \n        # Compute gradient\n        loss.backward()\n        \n        # Update perturbation\n        with torch.no_grad():\n            grad_sign = delta.grad.sign()\n            delta.data = delta.data + alpha * grad_sign\n            delta.data = torch.clamp(delta.data, -epsilon, epsilon)\n            delta.data = torch.clamp(images.data + delta.data, 0, 1) - images.data\n        \n        # Zero gradients for next iteration\n        delta.grad.zero_()\n    \n    # Restore training state\n    if was_training:\n        model.train()\n    \n    # Return adversarial images (detached)\n    return (images + delta).detach()\n\n\n# ==================== Triplet Dataset ====================\nclass TripletDataset(Dataset):\n    def __init__(self, dataset, train=True):\n        self.dataset = dataset\n        self.train = train\n        self.labels = np.array([label for _, label in dataset])\n        self.embeddings = None\n        \n        self.label_to_indices = defaultdict(list)\n        for idx, label in enumerate(self.labels):\n            self.label_to_indices[label].append(idx)\n        \n        self.labels_set = set(self.labels)\n        \n    def update_embeddings(self, embeddings):\n        self.embeddings = embeddings.detach().cpu()\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, index):\n        anchor_img, anchor_label = self.dataset[index]\n        \n        positive_indices = [idx for idx in self.label_to_indices[anchor_label] if idx != index]\n        if len(positive_indices) == 0:\n            positive_index = index\n        else:\n            if self.embeddings is not None and len(positive_indices) > 1:\n                anchor_emb = self.embeddings[index]\n                pos_embs = self.embeddings[positive_indices]\n                distances = torch.sum((anchor_emb - pos_embs) ** 2, dim=1)\n                k = max(1, int(len(distances) * 0.5))\n                topk_indices = torch.topk(distances, k, largest=True)[1]\n                selected = topk_indices[torch.randint(len(topk_indices), (1,))].item()\n                positive_index = positive_indices[selected]\n            else:\n                positive_index = np.random.choice(positive_indices)\n        \n        positive_img, _ = self.dataset[positive_index]\n        \n        if self.embeddings is not None:\n            anchor_emb = self.embeddings[index]\n            negative_candidates = []\n            for neg_label in (self.labels_set - {anchor_label}):\n                neg_indices = self.label_to_indices[neg_label]\n                sampled = np.random.choice(neg_indices, min(5, len(neg_indices)), replace=False)\n                negative_candidates.extend(sampled)\n            \n            if len(negative_candidates) > 0:\n                neg_embs = self.embeddings[negative_candidates]\n                distances = torch.sum((anchor_emb - neg_embs) ** 2, dim=1)\n                k = max(1, int(len(distances) * 0.3))\n                topk_indices = torch.topk(distances, k, largest=False)[1]\n                selected_idx = topk_indices[torch.randint(len(topk_indices), (1,))].item()\n                negative_index = negative_candidates[selected_idx]\n            else:\n                negative_label = np.random.choice(list(self.labels_set - {anchor_label}))\n                negative_index = np.random.choice(self.label_to_indices[negative_label])\n        else:\n            negative_label = np.random.choice(list(self.labels_set - {anchor_label}))\n            negative_index = np.random.choice(self.label_to_indices[negative_label])\n        \n        negative_img, _ = self.dataset[negative_index]\n        return anchor_img, positive_img, negative_img, anchor_label\n\n\nclass PredictionNetwork(nn.Module):\n    def __init__(self, embedding_dim=128, num_classes=10):\n        super(PredictionNetwork, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.dropout1 = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(256, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        return x\n\n\n# ==================== Training Functions ====================\ndef train_triplet_network(model, criterion, optimizer, train_loader, epoch, use_adv=False):\n    model.train()\n    total_loss = 0\n    total_intra = 0\n    total_inter = 0\n    total_adv = 0\n    \n    for batch_idx, (anchor, positive, negative, labels) in enumerate(train_loader):\n        anchor = anchor.to(device)\n        positive = positive.to(device)\n        negative = negative.to(device)\n        \n        optimizer.zero_grad()\n        \n        anchor_emb, positive_emb, negative_emb = model(anchor, positive, negative)\n        \n        # Adversarial samples (progressive schedule)\n        if use_adv and epoch > 12:\n            epsilon = 0.008 if epoch < 25 else 0.015 if epoch < 38 else 8/255\n            anchor_adv = generate_pgd_attack(model, anchor, epsilon=epsilon, num_iter=5)\n            positive_adv = generate_pgd_attack(model, positive, epsilon=epsilon, num_iter=5)\n            \n            anchor_adv_emb = get_embedding_safe(model, anchor_adv)\n            positive_adv_emb = get_embedding_safe(model, positive_adv)\n        else:\n            anchor_adv_emb = None\n            positive_adv_emb = None\n        \n        loss, intra_loss, inter_loss, adv_loss, lambda_k, gamma_k = criterion(\n            anchor_emb, positive_emb, negative_emb, anchor_adv_emb, positive_adv_emb\n        )\n        \n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"Warning: NaN/Inf at batch {batch_idx}, skipping...\")\n            continue\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        total_intra += intra_loss.item()\n        total_inter += inter_loss.item()\n        total_adv += adv_loss.item()\n        \n        if batch_idx % 75 == 0:\n            print(f'[{batch_idx:3d}/{len(train_loader)}] '\n                  f'L: {loss.item():.3f} | In: {intra_loss.item():.3f} | '\n                  f'Int: {inter_loss.item():.3f} | Adv: {adv_loss.item():.3f} | '\n                  f'Î»: {lambda_k:.2f} | Î³: {gamma_k:.2f}')\n    \n    return (total_loss / len(train_loader), total_intra / len(train_loader), \n            total_inter / len(train_loader), total_adv / len(train_loader))\n\n\ndef train_prediction_network(triplet_model, pred_network, optimizer, train_loader, epoch, use_adv=False):\n    triplet_model.eval()\n    pred_network.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    \n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        if use_adv and epoch > 8 and torch.rand(1).item() < 0.4:\n            images = generate_pgd_attack(triplet_model, images, epsilon=8/255, num_iter=10)\n        \n        optimizer.zero_grad()\n        \n        with torch.no_grad():\n            embeddings = get_embedding_safe(triplet_model, images)\n        \n        outputs = pred_network(embeddings)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(pred_network.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    return total_loss / len(train_loader), 100. * correct / total\n\n\ndef test_accuracy(triplet_model, pred_network, test_loader, use_pgd=False):\n    \"\"\"FIXED: PGD attack now works during evaluation\"\"\"\n    triplet_model.eval()\n    pred_network.eval()\n    correct = 0\n    total = 0\n    \n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        if use_pgd:\n            # PGD attack now works correctly even in eval mode\n            images = generate_pgd_attack(triplet_model, images, epsilon=8/255, num_iter=20)\n        \n        with torch.no_grad():\n            embeddings = get_embedding_safe(triplet_model, images)\n            outputs = pred_network(embeddings)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return 100. * correct / total\n\n\n# ==================== Main ====================\ndef main():\n    print(\"\\n\" + \"=\"*80)\n    print(\"ART-AL: Adversarially-Robust Triplet Networks (FULLY FIXED)\")\n    print(\"Target: Clean 78-82%, Robust 46-52% | Time: ~10-11 hours on 2xT4\")\n    print(\"=\"*80 + \"\\n\")\n    \n    BATCH_SIZE = 320\n    EMBEDDING_DIM = 128\n    NUM_EPOCHS_TRIPLET = 45\n    NUM_EPOCHS_PRED = 55\n    LR_TRIPLET = 0.0008\n    LR_PRED = 0.08\n    \n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    \n    print(\"Loading CIFAR-10...\")\n    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                                  download=True, transform=transform_train)\n    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                                 download=True, transform=transform_test)\n    \n    triplet_train_dataset = TripletDataset(train_dataset, train=True)\n    triplet_train_loader = DataLoader(triplet_train_dataset, batch_size=BATCH_SIZE,\n                                      shuffle=True, num_workers=4, pin_memory=True)\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                             shuffle=True, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                            shuffle=False, num_workers=4, pin_memory=True)\n    \n    print(\"Initializing models...\")\n    embedding_net = ResNet18Embedding(embedding_dim=EMBEDDING_DIM, dropout=0.1).to(device)\n    triplet_model = TripletNetwork(embedding_net).to(device)\n    pred_network = PredictionNetwork(embedding_dim=EMBEDDING_DIM, num_classes=10).to(device)\n    \n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n        triplet_model = nn.DataParallel(triplet_model)\n        pred_network = nn.DataParallel(pred_network)\n    \n    criterion = ARTALLoss(m1=0.5, m2=2.0)\n    optimizer_triplet = optim.AdamW(triplet_model.parameters(), lr=LR_TRIPLET, weight_decay=5e-4)\n    scheduler_triplet = optim.lr_scheduler.CosineAnnealingLR(optimizer_triplet, T_max=NUM_EPOCHS_TRIPLET)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"PHASE 1: Triplet Network Training with Progressive Adversarial Hardening\")\n    print(\"=\"*80)\n    start = time.time()\n    \n    for epoch in range(1, NUM_EPOCHS_TRIPLET + 1):\n        epoch_start = time.time()\n        avg_loss, avg_intra, avg_inter, avg_adv = train_triplet_network(\n            triplet_model, criterion, optimizer_triplet, triplet_train_loader, epoch, use_adv=True\n        )\n        scheduler_triplet.step()\n        \n        epoch_time = time.time() - epoch_start\n        print(f'\\nEpoch {epoch}/{NUM_EPOCHS_TRIPLET} ({epoch_time/60:.1f}min) | '\n              f'Loss: {avg_loss:.4f} | Intra: {avg_intra:.4f} | Inter: {avg_inter:.4f} | Adv: {avg_adv:.4f}')\n        \n        if epoch % 15 == 0:\n            print(\"Updating embeddings for hard mining...\")\n            triplet_model.eval()\n            all_embeddings = []\n            with torch.no_grad():\n                for images, _ in train_loader:\n                    images = images.to(device)\n                    emb = get_embedding_safe(triplet_model, images)\n                    all_embeddings.append(emb.cpu())\n            all_embeddings = torch.cat(all_embeddings)\n            triplet_train_dataset.update_embeddings(all_embeddings)\n            torch.save(triplet_model.state_dict(), f'checkpoint_triplet_{epoch}.pth')\n            print(f\"âœ“ Checkpoint saved\")\n    \n    phase1_time = time.time() - start\n    print(f\"\\nPhase 1 completed in {phase1_time/3600:.2f} hours\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"PHASE 2: Adversarial Partial Training (APT) of Prediction Network\")\n    print(\"=\"*80)\n    \n    for param in triplet_model.parameters():\n        param.requires_grad = False\n    triplet_model.eval()\n    \n    optimizer_pred = optim.SGD(pred_network.parameters(), lr=LR_PRED, \n                               momentum=0.9, weight_decay=5e-4, nesterov=True)\n    scheduler_pred = optim.lr_scheduler.CosineAnnealingLR(optimizer_pred, T_max=NUM_EPOCHS_PRED)\n    \n    best_clean = 0\n    best_robust = 0\n    best_epoch = 0\n    \n    for epoch in range(1, NUM_EPOCHS_PRED + 1):\n        train_loss, train_acc = train_prediction_network(\n            triplet_model, pred_network, optimizer_pred, train_loader, epoch, use_adv=True\n        )\n        scheduler_pred.step()\n        \n        if epoch % 5 == 0 or epoch == NUM_EPOCHS_PRED:\n            print(f\"Evaluating epoch {epoch}...\")\n            clean_acc = test_accuracy(triplet_model, pred_network, test_loader, use_pgd=False)\n            robust_acc = test_accuracy(triplet_model, pred_network, test_loader, use_pgd=True)\n            \n            print(f'Epoch {epoch:2d}/{NUM_EPOCHS_PRED} | Train: {train_acc:.1f}% | '\n                  f'Clean: {clean_acc:.2f}% | Robust(PGD-20): {robust_acc:.2f}%')\n            \n            if clean_acc + robust_acc > best_clean + best_robust:\n                best_clean = clean_acc\n                best_robust = robust_acc\n                best_epoch = epoch\n                torch.save({\n                    'epoch': epoch,\n                    'triplet_model': triplet_model.state_dict(),\n                    'pred_network': pred_network.state_dict(),\n                    'clean_acc': clean_acc,\n                    'robust_acc': robust_acc\n                }, 'best_artal_final.pth')\n                print(f'  âœ“ NEW BEST! Clean {clean_acc:.2f}% + Robust {robust_acc:.2f}%')\n        else:\n            print(f'Epoch {epoch:2d}/{NUM_EPOCHS_PRED} | Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n    \n    total_time = time.time() - start\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"FINAL RESULTS - ART-AL Framework\")\n    print(\"=\"*80)\n    print(f\"Best Clean Accuracy:  {best_clean:.2f}% (epoch {best_epoch})\")\n    print(f\"Best Robust Accuracy: {best_robust:.2f}% (PGD-20, Îµ=8/255)\")\n    print(f\"Combined Score:       {best_clean + best_robust:.2f}%\")\n    print(f\"Total Training Time:  {total_time/3600:.2f} hours\")\n    print(f\"\\nModel saved to: 'best_artal_final.pth'\")\n    print(\"=\"*80)\n    \n    if best_clean >= 78 and best_robust >= 46:\n        print(\"\\nðŸŽ‰ SUCCESS! Both targets achieved!\")\n        print(f\"   Clean:  {best_clean:.2f}% (target: â‰¥78%)\")\n        print(f\"   Robust: {best_robust:.2f}% (target: â‰¥46%)\")\n    else:\n        print(f\"\\nProgress toward targets:\")\n        print(f\"   Clean:  {best_clean:.2f}% / 78% (gap: {max(0, 78-best_clean):.2f}%)\")\n        print(f\"   Robust: {best_robust:.2f}% / 46% (gap: {max(0, 46-best_robust):.2f}%)\")\n    \n    return triplet_model, pred_network, best_clean, best_robust\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:00:56.434677Z","iopub.execute_input":"2025-12-18T07:00:56.434973Z","iopub.status.idle":"2025-12-18T15:05:38.448310Z","shell.execute_reply.started":"2025-12-18T07:00:56.434943Z","shell.execute_reply":"2025-12-18T15:05:38.447269Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla T4\nAvailable GPUs: 2\n\n================================================================================\nART-AL: Adversarially-Robust Triplet Networks (FULLY FIXED)\nTarget: Clean 78-82%, Robust 46-52% | Time: ~10-11 hours on 2xT4\n================================================================================\n\nLoading CIFAR-10...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:02<00:00, 72.5MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Initializing models...\nUsing 2 GPUs with DataParallel\n\n================================================================================\nPHASE 1: Triplet Network Training with Progressive Adversarial Hardening\n================================================================================\n[  0/157] L: 0.696 | In: 1.384 | Int: 0.189 | Adv: 0.000 | Î»: 0.58 | Î³: 0.00\n[ 75/157] L: 0.451 | In: 1.273 | Int: 0.245 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.421 | In: 1.253 | Int: 0.213 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 1/45 (1.1min) | Loss: 0.4550 | Intra: 1.2503 | Inter: 0.2461 | Adv: 0.0000\n[  0/157] L: 0.391 | In: 1.249 | Int: 0.176 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.416 | In: 1.266 | Int: 0.204 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.399 | In: 1.229 | Int: 0.191 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 2/45 (1.2min) | Loss: 0.4129 | Intra: 1.2637 | Inter: 0.2002 | Adv: 0.0000\n[  0/157] L: 0.421 | In: 1.220 | Int: 0.221 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.395 | In: 1.234 | Int: 0.185 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.370 | In: 1.196 | Int: 0.163 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 3/45 (1.2min) | Loss: 0.3985 | Intra: 1.2579 | Inter: 0.1836 | Adv: 0.0000\n[  0/157] L: 0.382 | In: 1.238 | Int: 0.167 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.410 | In: 1.217 | Int: 0.208 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.409 | In: 1.278 | Int: 0.192 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 4/45 (1.3min) | Loss: 0.3846 | Intra: 1.2290 | Inter: 0.1735 | Adv: 0.0000\n[  0/157] L: 0.382 | In: 1.242 | Int: 0.167 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.384 | In: 1.286 | Int: 0.159 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.358 | In: 1.241 | Int: 0.138 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 5/45 (1.3min) | Loss: 0.3788 | Intra: 1.2230 | Inter: 0.1677 | Adv: 0.0000\n[  0/157] L: 0.407 | In: 1.219 | Int: 0.204 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.343 | In: 1.176 | Int: 0.135 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.368 | In: 1.107 | Int: 0.184 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 6/45 (1.3min) | Loss: 0.3703 | Intra: 1.2044 | Inter: 0.1618 | Adv: 0.0000\n[  0/157] L: 0.368 | In: 1.189 | Int: 0.163 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.367 | In: 1.194 | Int: 0.160 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.400 | In: 1.233 | Int: 0.191 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 7/45 (1.3min) | Loss: 0.3645 | Intra: 1.1945 | Inter: 0.1570 | Adv: 0.0000\n[  0/157] L: 0.322 | In: 1.151 | Int: 0.115 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.370 | In: 1.148 | Int: 0.176 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.313 | In: 1.080 | Int: 0.121 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 8/45 (1.3min) | Loss: 0.3526 | Intra: 1.1480 | Inter: 0.1538 | Adv: 0.0000\n[  0/157] L: 0.371 | In: 1.153 | Int: 0.176 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.343 | In: 1.193 | Int: 0.131 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.338 | In: 1.068 | Int: 0.156 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 9/45 (1.3min) | Loss: 0.3473 | Intra: 1.1278 | Inter: 0.1522 | Adv: 0.0000\n[  0/157] L: 0.309 | In: 1.056 | Int: 0.122 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.354 | In: 1.189 | Int: 0.145 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.354 | In: 1.006 | Int: 0.192 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 10/45 (1.3min) | Loss: 0.3336 | Intra: 1.0897 | Inter: 0.1446 | Adv: 0.0000\n[  0/157] L: 0.357 | In: 1.091 | Int: 0.173 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.326 | In: 1.075 | Int: 0.139 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.321 | In: 1.047 | Int: 0.139 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 11/45 (1.3min) | Loss: 0.3202 | Intra: 1.0422 | Inter: 0.1398 | Adv: 0.0000\n[  0/157] L: 0.306 | In: 0.961 | Int: 0.142 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[ 75/157] L: 0.334 | In: 1.046 | Int: 0.156 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n[150/157] L: 0.330 | In: 1.031 | Int: 0.154 | Adv: 0.000 | Î»: 0.80 | Î³: 0.00\n\nEpoch 12/45 (1.3min) | Loss: 0.3105 | Intra: 1.0066 | Inter: 0.1365 | Adv: 0.0000\n[  0/157] L: 0.279 | In: 0.879 | Int: 0.105 | Adv: 1.509 | Î»: 0.80 | Î³: 0.01\n[ 75/157] L: 0.318 | In: 1.007 | Int: 0.214 | Adv: 0.189 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.289 | In: 1.072 | Int: 0.185 | Adv: 0.119 | Î»: 0.80 | Î³: 0.30\n\nEpoch 13/45 (10.0min) | Loss: 0.3426 | Intra: 1.0442 | Inter: 0.2002 | Adv: 0.3493\n[  0/157] L: 0.296 | In: 1.082 | Int: 0.193 | Adv: 0.123 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.302 | In: 1.137 | Int: 0.183 | Adv: 0.134 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.288 | In: 1.114 | Int: 0.179 | Adv: 0.105 | Î»: 0.80 | Î³: 0.30\n\nEpoch 14/45 (10.0min) | Loss: 0.2789 | Intra: 1.0873 | Inter: 0.1637 | Adv: 0.1167\n[  0/157] L: 0.287 | In: 1.143 | Int: 0.176 | Adv: 0.097 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.284 | In: 1.050 | Int: 0.189 | Adv: 0.103 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.274 | In: 1.052 | Int: 0.177 | Adv: 0.092 | Î»: 0.80 | Î³: 0.30\n\nEpoch 15/45 (10.0min) | Loss: 0.2681 | Intra: 1.0608 | Inter: 0.1587 | Adv: 0.1024\nUpdating embeddings for hard mining...\nâœ“ Checkpoint saved\n[  0/157] L: 0.266 | In: 1.012 | Int: 0.175 | Adv: 0.087 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.250 | In: 0.971 | Int: 0.154 | Adv: 0.093 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.247 | In: 1.008 | Int: 0.143 | Adv: 0.088 | Î»: 0.80 | Î³: 0.30\n\nEpoch 16/45 (10.0min) | Loss: 0.2564 | Intra: 1.0301 | Inter: 0.1497 | Adv: 0.0943\n[  0/157] L: 0.259 | In: 1.009 | Int: 0.162 | Adv: 0.090 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.235 | In: 0.977 | Int: 0.133 | Adv: 0.080 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.245 | In: 0.995 | Int: 0.140 | Adv: 0.090 | Î»: 0.80 | Î³: 0.30\n\nEpoch 17/45 (10.0min) | Loss: 0.2495 | Intra: 0.9948 | Inter: 0.1479 | Adv: 0.0914\n[  0/157] L: 0.236 | In: 0.905 | Int: 0.154 | Adv: 0.075 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.262 | In: 1.001 | Int: 0.166 | Adv: 0.095 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.216 | In: 0.939 | Int: 0.103 | Adv: 0.091 | Î»: 0.80 | Î³: 0.30\n\nEpoch 18/45 (10.0min) | Loss: 0.2410 | Intra: 0.9665 | Inter: 0.1413 | Adv: 0.0887\n[  0/157] L: 0.235 | In: 0.963 | Int: 0.131 | Adv: 0.089 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.236 | In: 0.950 | Int: 0.142 | Adv: 0.079 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.240 | In: 0.978 | Int: 0.137 | Adv: 0.087 | Î»: 0.80 | Î³: 0.30\n\nEpoch 19/45 (10.0min) | Loss: 0.2357 | Intra: 0.9501 | Inter: 0.1368 | Adv: 0.0868\n[  0/157] L: 0.248 | In: 0.903 | Int: 0.171 | Adv: 0.086 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.221 | In: 0.925 | Int: 0.118 | Adv: 0.084 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.234 | In: 0.882 | Int: 0.152 | Adv: 0.083 | Î»: 0.80 | Î³: 0.30\n\nEpoch 20/45 (10.1min) | Loss: 0.2267 | Intra: 0.9130 | Inter: 0.1302 | Adv: 0.0865\n[  0/157] L: 0.222 | In: 0.848 | Int: 0.136 | Adv: 0.090 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.233 | In: 0.972 | Int: 0.132 | Adv: 0.077 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.225 | In: 0.896 | Int: 0.134 | Adv: 0.080 | Î»: 0.80 | Î³: 0.30\n\nEpoch 21/45 (10.0min) | Loss: 0.2202 | Intra: 0.8830 | Inter: 0.1271 | Adv: 0.0847\n[  0/157] L: 0.225 | In: 0.916 | Int: 0.127 | Adv: 0.085 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.200 | In: 0.837 | Int: 0.104 | Adv: 0.082 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.227 | In: 0.893 | Int: 0.136 | Adv: 0.087 | Î»: 0.80 | Î³: 0.30\n\nEpoch 22/45 (10.0min) | Loss: 0.2167 | Intra: 0.8688 | Inter: 0.1240 | Adv: 0.0854\n[  0/157] L: 0.214 | In: 0.869 | Int: 0.116 | Adv: 0.092 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.214 | In: 0.894 | Int: 0.118 | Adv: 0.076 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.216 | In: 0.798 | Int: 0.133 | Adv: 0.099 | Î»: 0.80 | Î³: 0.30\n\nEpoch 23/45 (10.0min) | Loss: 0.2113 | Intra: 0.8410 | Inter: 0.1222 | Adv: 0.0836\n[  0/157] L: 0.223 | In: 0.830 | Int: 0.137 | Adv: 0.102 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.216 | In: 0.798 | Int: 0.147 | Adv: 0.073 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.207 | In: 0.822 | Int: 0.117 | Adv: 0.087 | Î»: 0.80 | Î³: 0.30\n\nEpoch 24/45 (10.0min) | Loss: 0.2057 | Intra: 0.8141 | Inter: 0.1201 | Adv: 0.0815\n[  0/157] L: 0.188 | In: 0.781 | Int: 0.097 | Adv: 0.083 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.195 | In: 0.771 | Int: 0.117 | Adv: 0.071 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.208 | In: 0.829 | Int: 0.108 | Adv: 0.103 | Î»: 0.80 | Î³: 0.30\n\nEpoch 25/45 (10.0min) | Loss: 0.2020 | Intra: 0.7907 | Inter: 0.1175 | Adv: 0.0851\n[  0/157] L: 0.202 | In: 0.817 | Int: 0.108 | Adv: 0.090 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.200 | In: 0.751 | Int: 0.114 | Adv: 0.103 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.205 | In: 0.789 | Int: 0.121 | Adv: 0.091 | Î»: 0.80 | Î³: 0.30\n\nEpoch 26/45 (10.0min) | Loss: 0.1980 | Intra: 0.7760 | Inter: 0.1140 | Adv: 0.0851\n[  0/157] L: 0.190 | In: 0.780 | Int: 0.099 | Adv: 0.083 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.199 | In: 0.745 | Int: 0.125 | Adv: 0.084 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.189 | In: 0.716 | Int: 0.117 | Adv: 0.077 | Î»: 0.80 | Î³: 0.30\n\nEpoch 27/45 (10.0min) | Loss: 0.1937 | Intra: 0.7527 | Inter: 0.1125 | Adv: 0.0843\n[  0/157] L: 0.204 | In: 0.675 | Int: 0.151 | Adv: 0.081 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.192 | In: 0.752 | Int: 0.116 | Adv: 0.070 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.183 | In: 0.734 | Int: 0.103 | Adv: 0.076 | Î»: 0.80 | Î³: 0.30\n\nEpoch 28/45 (10.0min) | Loss: 0.1916 | Intra: 0.7378 | Inter: 0.1128 | Adv: 0.0838\n[  0/157] L: 0.185 | In: 0.661 | Int: 0.122 | Adv: 0.081 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.196 | In: 0.729 | Int: 0.124 | Adv: 0.082 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.178 | In: 0.723 | Int: 0.093 | Adv: 0.082 | Î»: 0.80 | Î³: 0.30\n\nEpoch 29/45 (10.0min) | Loss: 0.1861 | Intra: 0.7107 | Inter: 0.1092 | Adv: 0.0849\n[  0/157] L: 0.176 | In: 0.631 | Int: 0.113 | Adv: 0.081 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.224 | In: 0.741 | Int: 0.160 | Adv: 0.102 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.167 | In: 0.663 | Int: 0.086 | Adv: 0.088 | Î»: 0.80 | Î³: 0.30\n\nEpoch 30/45 (10.0min) | Loss: 0.1819 | Intra: 0.6903 | Inter: 0.1068 | Adv: 0.0850\nUpdating embeddings for hard mining...\nâœ“ Checkpoint saved\n[  0/157] L: 0.186 | In: 0.682 | Int: 0.112 | Adv: 0.092 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.172 | In: 0.690 | Int: 0.091 | Adv: 0.082 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.191 | In: 0.762 | Int: 0.098 | Adv: 0.097 | Î»: 0.80 | Î³: 0.30\n\nEpoch 31/45 (10.1min) | Loss: 0.1774 | Intra: 0.6708 | Inter: 0.1036 | Adv: 0.0850\n[  0/157] L: 0.151 | In: 0.616 | Int: 0.078 | Adv: 0.069 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.152 | In: 0.622 | Int: 0.073 | Adv: 0.081 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.164 | In: 0.582 | Int: 0.107 | Adv: 0.076 | Î»: 0.80 | Î³: 0.30\n\nEpoch 32/45 (10.0min) | Loss: 0.1752 | Intra: 0.6567 | Inter: 0.1037 | Adv: 0.0840\n[  0/157] L: 0.158 | In: 0.593 | Int: 0.089 | Adv: 0.084 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.163 | In: 0.667 | Int: 0.081 | Adv: 0.082 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.176 | In: 0.657 | Int: 0.105 | Adv: 0.086 | Î»: 0.80 | Î³: 0.30\n\nEpoch 33/45 (10.1min) | Loss: 0.1702 | Intra: 0.6320 | Inter: 0.1002 | Adv: 0.0853\n[  0/157] L: 0.165 | In: 0.605 | Int: 0.091 | Adv: 0.099 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.178 | In: 0.597 | Int: 0.115 | Adv: 0.099 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.177 | In: 0.607 | Int: 0.112 | Adv: 0.096 | Î»: 0.80 | Î³: 0.30\n\nEpoch 34/45 (10.1min) | Loss: 0.1685 | Intra: 0.6195 | Inter: 0.1004 | Adv: 0.0850\n[  0/157] L: 0.188 | In: 0.621 | Int: 0.132 | Adv: 0.090 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.156 | In: 0.551 | Int: 0.095 | Adv: 0.086 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.166 | In: 0.606 | Int: 0.101 | Adv: 0.081 | Î»: 0.80 | Î³: 0.30\n\nEpoch 35/45 (10.1min) | Loss: 0.1658 | Intra: 0.6115 | Inter: 0.0980 | Adv: 0.0843\n[  0/157] L: 0.159 | In: 0.623 | Int: 0.085 | Adv: 0.082 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.156 | In: 0.557 | Int: 0.093 | Adv: 0.086 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.180 | In: 0.586 | Int: 0.123 | Adv: 0.097 | Î»: 0.80 | Î³: 0.30\n\nEpoch 36/45 (10.1min) | Loss: 0.1620 | Intra: 0.5881 | Inter: 0.0972 | Adv: 0.0843\n[  0/157] L: 0.167 | In: 0.595 | Int: 0.096 | Adv: 0.099 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.151 | In: 0.542 | Int: 0.095 | Adv: 0.073 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.164 | In: 0.560 | Int: 0.113 | Adv: 0.075 | Î»: 0.80 | Î³: 0.30\n\nEpoch 37/45 (10.1min) | Loss: 0.1583 | Intra: 0.5749 | Inter: 0.0937 | Adv: 0.0845\n[  0/157] L: 0.164 | In: 0.594 | Int: 0.088 | Adv: 0.105 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.183 | In: 0.592 | Int: 0.134 | Adv: 0.082 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.163 | In: 0.561 | Int: 0.100 | Adv: 0.094 | Î»: 0.80 | Î³: 0.30\n\nEpoch 38/45 (10.0min) | Loss: 0.1599 | Intra: 0.5656 | Inter: 0.0944 | Adv: 0.0928\n[  0/157] L: 0.150 | In: 0.571 | Int: 0.076 | Adv: 0.091 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.150 | In: 0.595 | Int: 0.068 | Adv: 0.096 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.158 | In: 0.552 | Int: 0.098 | Adv: 0.086 | Î»: 0.80 | Î³: 0.30\n\nEpoch 39/45 (10.0min) | Loss: 0.1587 | Intra: 0.5610 | Inter: 0.0942 | Adv: 0.0913\n[  0/157] L: 0.171 | In: 0.613 | Int: 0.092 | Adv: 0.111 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.159 | In: 0.512 | Int: 0.106 | Adv: 0.094 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.153 | In: 0.547 | Int: 0.095 | Adv: 0.078 | Î»: 0.80 | Î³: 0.30\n\nEpoch 40/45 (10.0min) | Loss: 0.1569 | Intra: 0.5534 | Inter: 0.0926 | Adv: 0.0919\n[  0/157] L: 0.152 | In: 0.577 | Int: 0.082 | Adv: 0.085 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.150 | In: 0.465 | Int: 0.104 | Adv: 0.089 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.165 | In: 0.626 | Int: 0.092 | Adv: 0.086 | Î»: 0.80 | Î³: 0.30\n\nEpoch 41/45 (10.0min) | Loss: 0.1566 | Intra: 0.5540 | Inter: 0.0927 | Adv: 0.0906\n[  0/157] L: 0.169 | In: 0.525 | Int: 0.113 | Adv: 0.106 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.158 | In: 0.535 | Int: 0.103 | Adv: 0.086 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.137 | In: 0.518 | Int: 0.072 | Adv: 0.081 | Î»: 0.80 | Î³: 0.30\n\nEpoch 42/45 (10.0min) | Loss: 0.1551 | Intra: 0.5464 | Inter: 0.0920 | Adv: 0.0902\n[  0/157] L: 0.160 | In: 0.542 | Int: 0.099 | Adv: 0.095 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.146 | In: 0.537 | Int: 0.075 | Adv: 0.098 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.166 | In: 0.570 | Int: 0.112 | Adv: 0.078 | Î»: 0.80 | Î³: 0.30\n\nEpoch 43/45 (10.0min) | Loss: 0.1541 | Intra: 0.5448 | Inter: 0.0908 | Adv: 0.0899\n[  0/157] L: 0.156 | In: 0.527 | Int: 0.094 | Adv: 0.099 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.179 | In: 0.594 | Int: 0.113 | Adv: 0.109 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.151 | In: 0.558 | Int: 0.086 | Adv: 0.083 | Î»: 0.80 | Î³: 0.30\n\nEpoch 44/45 (10.1min) | Loss: 0.1523 | Intra: 0.5446 | Inter: 0.0883 | Adv: 0.0887\n[  0/157] L: 0.164 | In: 0.532 | Int: 0.108 | Adv: 0.096 | Î»: 0.80 | Î³: 0.30\n[ 75/157] L: 0.141 | In: 0.530 | Int: 0.069 | Adv: 0.093 | Î»: 0.80 | Î³: 0.30\n[150/157] L: 0.158 | In: 0.528 | Int: 0.104 | Adv: 0.085 | Î»: 0.80 | Î³: 0.30\n\nEpoch 45/45 (10.1min) | Loss: 0.1547 | Intra: 0.5440 | Inter: 0.0922 | Adv: 0.0898\nUpdating embeddings for hard mining...\nâœ“ Checkpoint saved\n\nPhase 1 completed in 5.78 hours\n\n================================================================================\nPHASE 2: Adversarial Partial Training (APT) of Prediction Network\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n  return F.linear(input, self.weight, self.bias)\n","output_type":"stream"},{"name":"stdout","text":"Epoch  1/55 | Loss: 1.054 | Train Acc: 76.71%\nEpoch  2/55 | Loss: 1.009 | Train Acc: 77.78%\nEpoch  3/55 | Loss: 1.001 | Train Acc: 78.15%\nEpoch  4/55 | Loss: 0.999 | Train Acc: 77.97%\nEvaluating epoch 5...\nEpoch  5/55 | Train: 78.1% | Clean: 76.69% | Robust(PGD-20): 68.66%\n  âœ“ NEW BEST! Clean 76.69% + Robust 68.66%\nEpoch  6/55 | Loss: 0.994 | Train Acc: 78.40%\nEpoch  7/55 | Loss: 0.993 | Train Acc: 78.45%\nEpoch  8/55 | Loss: 0.992 | Train Acc: 78.40%\nEpoch  9/55 | Loss: 1.060 | Train Acc: 75.39%\nEvaluating epoch 10...\nEpoch 10/55 | Train: 75.6% | Clean: 76.51% | Robust(PGD-20): 69.10%\n  âœ“ NEW BEST! Clean 76.51% + Robust 69.10%\nEpoch 11/55 | Loss: 1.054 | Train Acc: 75.66%\nEpoch 12/55 | Loss: 1.064 | Train Acc: 75.22%\nEpoch 13/55 | Loss: 1.056 | Train Acc: 75.42%\nEpoch 14/55 | Loss: 1.061 | Train Acc: 75.18%\nEvaluating epoch 15...\nEpoch 15/55 | Train: 75.3% | Clean: 76.66% | Robust(PGD-20): 69.37%\n  âœ“ NEW BEST! Clean 76.66% + Robust 69.37%\nEpoch 16/55 | Loss: 1.063 | Train Acc: 75.24%\nEpoch 17/55 | Loss: 1.061 | Train Acc: 75.22%\nEpoch 18/55 | Loss: 1.051 | Train Acc: 75.78%\nEpoch 19/55 | Loss: 1.055 | Train Acc: 75.55%\nEvaluating epoch 20...\nEpoch 20/55 | Train: 75.6% | Clean: 76.48% | Robust(PGD-20): 69.89%\n  âœ“ NEW BEST! Clean 76.48% + Robust 69.89%\nEpoch 21/55 | Loss: 1.047 | Train Acc: 75.79%\nEpoch 22/55 | Loss: 1.055 | Train Acc: 75.44%\nEpoch 23/55 | Loss: 1.045 | Train Acc: 76.00%\nEpoch 24/55 | Loss: 1.058 | Train Acc: 75.43%\nEvaluating epoch 25...\nEpoch 25/55 | Train: 76.0% | Clean: 76.77% | Robust(PGD-20): 69.66%\n  âœ“ NEW BEST! Clean 76.77% + Robust 69.66%\nEpoch 26/55 | Loss: 1.056 | Train Acc: 75.52%\nEpoch 27/55 | Loss: 1.059 | Train Acc: 75.17%\nEpoch 28/55 | Loss: 1.063 | Train Acc: 75.17%\nEpoch 29/55 | Loss: 1.041 | Train Acc: 76.19%\nEvaluating epoch 30...\nEpoch 30/55 | Train: 75.3% | Clean: 76.67% | Robust(PGD-20): 69.75%\nEpoch 31/55 | Loss: 1.060 | Train Acc: 75.36%\nEpoch 32/55 | Loss: 1.061 | Train Acc: 75.19%\nEpoch 33/55 | Loss: 1.049 | Train Acc: 75.64%\nEpoch 34/55 | Loss: 1.053 | Train Acc: 75.34%\nEvaluating epoch 35...\nEpoch 35/55 | Train: 76.1% | Clean: 76.87% | Robust(PGD-20): 70.13%\n  âœ“ NEW BEST! Clean 76.87% + Robust 70.13%\nEpoch 36/55 | Loss: 1.052 | Train Acc: 75.63%\nEpoch 37/55 | Loss: 1.053 | Train Acc: 75.49%\nEpoch 38/55 | Loss: 1.041 | Train Acc: 76.05%\nEpoch 39/55 | Loss: 1.046 | Train Acc: 75.78%\nEvaluating epoch 40...\nEpoch 40/55 | Train: 75.6% | Clean: 76.68% | Robust(PGD-20): 70.17%\nEpoch 41/55 | Loss: 1.047 | Train Acc: 75.81%\nEpoch 42/55 | Loss: 1.046 | Train Acc: 76.00%\nEpoch 43/55 | Loss: 1.034 | Train Acc: 76.41%\nEpoch 44/55 | Loss: 1.051 | Train Acc: 75.70%\nEvaluating epoch 45...\nEpoch 45/55 | Train: 75.3% | Clean: 76.77% | Robust(PGD-20): 70.25%\n  âœ“ NEW BEST! Clean 76.77% + Robust 70.25%\nEpoch 46/55 | Loss: 1.045 | Train Acc: 75.87%\nEpoch 47/55 | Loss: 1.032 | Train Acc: 76.51%\nEpoch 48/55 | Loss: 1.050 | Train Acc: 75.50%\nEpoch 49/55 | Loss: 1.055 | Train Acc: 75.33%\nEvaluating epoch 50...\nEpoch 50/55 | Train: 76.0% | Clean: 76.80% | Robust(PGD-20): 69.93%\nEpoch 51/55 | Loss: 1.047 | Train Acc: 75.75%\nEpoch 52/55 | Loss: 1.053 | Train Acc: 75.31%\nEpoch 53/55 | Loss: 1.045 | Train Acc: 75.88%\nEpoch 54/55 | Loss: 1.036 | Train Acc: 76.08%\nEvaluating epoch 55...\nEpoch 55/55 | Train: 75.5% | Clean: 76.77% | Robust(PGD-20): 70.19%\n\n================================================================================\nFINAL RESULTS - ART-AL Framework\n================================================================================\nBest Clean Accuracy:  76.77% (epoch 45)\nBest Robust Accuracy: 70.25% (PGD-20, Îµ=8/255)\nCombined Score:       147.02%\nTotal Training Time:  8.07 hours\n\nModel saved to: 'best_artal_final.pth'\n================================================================================\n\nProgress toward targets:\n   Clean:  76.77% / 78% (gap: 1.23%)\n   Robust: 70.25% / 46% (gap: 0.00%)\n","output_type":"stream"}],"execution_count":1}]}