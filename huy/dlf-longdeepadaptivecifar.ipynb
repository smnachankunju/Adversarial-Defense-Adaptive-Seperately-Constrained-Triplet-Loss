{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom collections import defaultdict\nimport random\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\n# ==================== ResNet Building Blocks ====================\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                               stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels * self.expansion:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels * self.expansion,\n                         kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * self.expansion)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNetEmbedding(nn.Module):\n    def __init__(self, block, num_blocks, embedding_dim=128, dropout=0.1):\n        super(ResNetEmbedding, self).__init__()\n        self.in_channels = 64\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.dropout = nn.Dropout2d(dropout)\n        \n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, embedding_dim)\n        self.bn_emb = nn.BatchNorm1d(embedding_dim)\n        \n    def _make_layer(self, block, out_channels, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels * block.expansion\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.dropout(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        out = self.bn_emb(out)\n        return out\n\n\ndef ResNet18Embedding(embedding_dim=128, dropout=0.1):\n    return ResNetEmbedding(BasicBlock, [2, 2, 2, 2], embedding_dim, dropout)\n\n\n# ==================== Triplet Network ====================\nclass TripletNetwork(nn.Module):\n    def __init__(self, embedding_net):\n        super(TripletNetwork, self).__init__()\n        self.embedding_net = embedding_net\n        \n    def forward(self, anchor, positive, negative):\n        anchor_embedding = self.embedding_net(anchor)\n        positive_embedding = self.embedding_net(positive)\n        negative_embedding = self.embedding_net(negative)\n        return anchor_embedding, positive_embedding, negative_embedding\n    \n    def get_embedding(self, x):\n        return self.embedding_net(x)\n\n\n# ==================== CORRECTED A-SCTL Loss ====================\nclass ASCTLLoss(nn.Module):\n    def __init__(self, m1=0.01, m2=1.0, lambda_min=0.1, lambda_max=0.9):\n        super(ASCTLLoss, self).__init__()\n        self.m1 = m1  # Paper's value: 0.01\n        self.m2 = m2  # Paper's value: 1.0\n        self.lambda_min = lambda_min\n        self.lambda_max = lambda_max\n        self.lambda_k = 0.5\n        \n    def forward(self, anchor, positive, negative):\n        # Squared Euclidean distances\n        pos_dist = torch.sum((anchor - positive) ** 2, dim=1)\n        neg_dist = torch.sum((anchor - negative) ** 2, dim=1)\n        \n        # Losses per paper\n        intra_loss = torch.clamp(pos_dist - self.m1, min=0.0)\n        inter_loss = torch.clamp(self.m2 - neg_dist, min=0.0)\n        \n        intra_loss_mean = torch.mean(intra_loss)\n        inter_loss_mean = torch.mean(inter_loss)\n        \n        # Adaptive lambda - more responsive than before\n        total_loss = intra_loss_mean.item() + inter_loss_mean.item()\n        if total_loss > 1e-8:\n            new_lambda = intra_loss_mean.item() / total_loss\n            # Reduced smoothing for faster adaptation\n            self.lambda_k = 0.6 * self.lambda_k + 0.4 * new_lambda\n            self.lambda_k = max(self.lambda_min, min(self.lambda_max, self.lambda_k))\n        \n        loss = (1 - self.lambda_k) * intra_loss_mean + self.lambda_k * inter_loss_mean\n        \n        return loss, intra_loss_mean, inter_loss_mean, self.lambda_k\n\n\n# ==================== Enhanced Triplet Dataset with Hard Mining ====================\nclass TripletDataset(Dataset):\n    def __init__(self, dataset, train=True):\n        self.dataset = dataset\n        self.train = train\n        self.labels = np.array([label for _, label in dataset])\n        self.embeddings = None\n        \n        self.label_to_indices = defaultdict(list)\n        for idx, label in enumerate(self.labels):\n            self.label_to_indices[label].append(idx)\n        \n        self.labels_set = set(self.labels)\n        \n    def update_embeddings(self, embeddings):\n        \"\"\"Update embeddings for hard negative mining\"\"\"\n        self.embeddings = embeddings.detach().cpu()\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, index):\n        anchor_img, anchor_label = self.dataset[index]\n        \n        # Positive - same class, different sample\n        positive_index = index\n        while positive_index == index:\n            positive_index = np.random.choice(self.label_to_indices[anchor_label])\n        positive_img, _ = self.dataset[positive_index]\n        \n        # Hard/Semi-Hard Negative Mining\n        if self.embeddings is not None and len(self.embeddings) > 0:\n            anchor_emb = self.embeddings[index]\n            \n            # Collect candidates from all other classes\n            negative_candidates = []\n            for neg_label in (self.labels_set - {anchor_label}):\n                neg_indices = self.label_to_indices[neg_label]\n                # Sample subset for efficiency\n                sampled = np.random.choice(neg_indices, min(10, len(neg_indices)), replace=False)\n                negative_candidates.extend(sampled)\n            \n            if len(negative_candidates) > 0:\n                neg_embs = self.embeddings[negative_candidates]\n                distances = torch.sum((anchor_emb - neg_embs) ** 2, dim=1)\n                \n                # Semi-hard mining: select from top 30% hardest negatives\n                # (not the absolute hardest to avoid outliers)\n                k = max(1, int(len(distances) * 0.3))\n                topk_indices = torch.topk(distances, k, largest=False)[1]\n                selected_idx = topk_indices[torch.randint(len(topk_indices), (1,))]\n                negative_index = negative_candidates[selected_idx.item()]\n            else:\n                # Fallback to random\n                negative_label = np.random.choice(list(self.labels_set - {anchor_label}))\n                negative_index = np.random.choice(self.label_to_indices[negative_label])\n        else:\n            # Random sampling (used in early epochs)\n            negative_label = np.random.choice(list(self.labels_set - {anchor_label}))\n            negative_index = np.random.choice(self.label_to_indices[negative_label])\n        \n        negative_img, _ = self.dataset[negative_index]\n        return anchor_img, positive_img, negative_img, anchor_label\n\n\n# ==================== Improved Prediction Network ====================\nclass PredictionNetwork(nn.Module):\n    def __init__(self, embedding_dim=128, num_classes=10):\n        super(PredictionNetwork, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(0.3)\n        \n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.dropout2 = nn.Dropout(0.2)\n        \n        self.fc3 = nn.Linear(256, num_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        x = self.fc3(x)\n        return x\n\n\n# ==================== Embedding Quality Metrics ====================\ndef compute_embedding_quality(model, dataloader):\n    \"\"\"Compute intra-class and inter-class distances\"\"\"\n    model.eval()\n    embeddings = []\n    labels_list = []\n    \n    with torch.no_grad():\n        for images, labels in dataloader:\n            images = images.to(device)\n            emb = model.get_embedding(images)\n            embeddings.append(emb.cpu())\n            labels_list.append(labels)\n    \n    embeddings = torch.cat(embeddings)\n    labels_list = torch.cat(labels_list)\n    \n    # Compute intra-class distance\n    intra_distances = []\n    for label in torch.unique(labels_list):\n        mask = labels_list == label\n        class_emb = embeddings[mask]\n        if len(class_emb) > 1:\n            dists = torch.cdist(class_emb, class_emb, p=2)\n            intra_distances.append(dists[torch.triu(torch.ones_like(dists), diagonal=1) == 1].mean())\n    \n    # Compute inter-class distance\n    inter_distances = []\n    unique_labels = torch.unique(labels_list)\n    for i, label1 in enumerate(unique_labels):\n        for label2 in unique_labels[i+1:]:\n            emb1 = embeddings[labels_list == label1]\n            emb2 = embeddings[labels_list == label2]\n            dists = torch.cdist(emb1, emb2, p=2)\n            inter_distances.append(dists.mean())\n    \n    avg_intra = torch.tensor(intra_distances).mean().item() if intra_distances else 0\n    avg_inter = torch.tensor(inter_distances).mean().item() if inter_distances else 0\n    \n    return avg_intra, avg_inter\n\n\n# ==================== Training Functions ====================\ndef train_triplet_network(model, criterion, optimizer, train_loader, epoch):\n    model.train()\n    total_loss = 0\n    total_intra = 0\n    total_inter = 0\n    lambda_values = []\n    \n    for batch_idx, (anchor, positive, negative, labels) in enumerate(train_loader):\n        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n        \n        optimizer.zero_grad()\n        \n        anchor_emb, positive_emb, negative_emb = model(anchor, positive, negative)\n        \n        loss, intra_loss, inter_loss, lambda_k = criterion(anchor_emb, positive_emb, negative_emb)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        total_intra += intra_loss.item()\n        total_inter += inter_loss.item()\n        lambda_values.append(lambda_k)\n        \n        if batch_idx % 100 == 0:\n            print(f'Epoch {epoch} [{batch_idx}/{len(train_loader)}] '\n                  f'Loss: {loss.item():.4f} | Intra: {intra_loss.item():.4f} | '\n                  f'Inter: {inter_loss.item():.4f} | Î»: {lambda_k:.3f}')\n    \n    avg_loss = total_loss / len(train_loader)\n    avg_intra = total_intra / len(train_loader)\n    avg_inter = total_inter / len(train_loader)\n    avg_lambda = np.mean(lambda_values)\n    \n    return avg_loss, avg_intra, avg_inter, avg_lambda\n\n\ndef train_prediction_network(triplet_model, pred_network, optimizer, train_loader, epoch):\n    triplet_model.eval()\n    pred_network.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        with torch.no_grad():\n            embeddings = triplet_model.get_embedding(images)\n        \n        outputs = pred_network(embeddings)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(pred_network.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    accuracy = 100. * correct / total\n    avg_loss = total_loss / len(train_loader)\n    \n    return avg_loss, accuracy\n\n\ndef test_classification(triplet_model, pred_network, test_loader):\n    triplet_model.eval()\n    pred_network.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            embeddings = triplet_model.get_embedding(images)\n            outputs = pred_network(embeddings)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    accuracy = 100. * correct / total\n    return accuracy\n\n\n# ==================== Main ====================\ndef main():\n    # Optimized hyperparameters\n    BATCH_SIZE = 128\n    EMBEDDING_DIM = 128  # Paper's optimal value\n    NUM_EPOCHS_TRIPLET = 80  # Increased for better convergence\n    NUM_EPOCHS_PRED = 100\n    LEARNING_RATE_TRIPLET = 0.001\n    LEARNING_RATE_PRED = 0.1  # Higher initial LR with better scheduler\n    \n    # Stronger augmentation for better generalization\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    \n    print(\"Loading CIFAR-10 dataset...\")\n    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                                  download=True, transform=transform_train)\n    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                                 download=True, transform=transform_test)\n    \n    triplet_train_dataset = TripletDataset(train_dataset, train=True)\n    triplet_train_loader = DataLoader(triplet_train_dataset, batch_size=BATCH_SIZE,\n                                      shuffle=True, num_workers=4, pin_memory=True)\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                             shuffle=True, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                            shuffle=False, num_workers=4, pin_memory=True)\n    \n    print(\"\\nInitializing ResNet-18 Triplet Network with A-SCTL...\")\n    embedding_net = ResNet18Embedding(embedding_dim=EMBEDDING_DIM, dropout=0.1).to(device)\n    triplet_model = TripletNetwork(embedding_net).to(device)\n    pred_network = PredictionNetwork(embedding_dim=EMBEDDING_DIM, num_classes=10).to(device)\n    \n    # CORRECTED: Paper's margin values\n    triplet_criterion = ASCTLLoss(m1=0.01, m2=1.0, lambda_min=0.1, lambda_max=0.9)\n    triplet_optimizer = optim.AdamW(triplet_model.parameters(), lr=LEARNING_RATE_TRIPLET, weight_decay=5e-4)\n    triplet_scheduler = optim.lr_scheduler.CosineAnnealingLR(triplet_optimizer, T_max=NUM_EPOCHS_TRIPLET)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"PHASE 1: Training Triplet Network with A-SCTL\")\n    print(\"=\"*70)\n    \n    best_triplet_loss = float('inf')\n    for epoch in range(1, NUM_EPOCHS_TRIPLET + 1):\n        avg_loss, avg_intra, avg_inter, avg_lambda = train_triplet_network(\n            triplet_model, triplet_criterion, triplet_optimizer, triplet_train_loader, epoch\n        )\n        triplet_scheduler.step()\n        \n        print(f'Epoch {epoch:2d} | Loss: {avg_loss:.4f} | Intra: {avg_intra:.4f} | '\n              f'Inter: {avg_inter:.4f} | Î»: {avg_lambda:.4f}')\n        \n        # Update embeddings for hard mining every 10 epochs\n        if epoch % 10 == 0:\n            print(\"Updating embeddings for hard negative mining...\")\n            triplet_model.eval()\n            all_embeddings = []\n            with torch.no_grad():\n                for images, _ in train_loader:\n                    images = images.to(device)\n                    emb = triplet_model.get_embedding(images)\n                    all_embeddings.append(emb.cpu())\n            all_embeddings = torch.cat(all_embeddings)\n            triplet_train_dataset.update_embeddings(all_embeddings)\n            print(f\"âœ“ Embeddings updated (epoch {epoch})\")\n        \n        if avg_loss < best_triplet_loss:\n            best_triplet_loss = avg_loss\n            torch.save(triplet_model.state_dict(), 'best_triplet_embeddings.pth')\n    \n    # Evaluate embedding quality\n    print(\"\\n\" + \"=\"*70)\n    print(\"Evaluating Embedding Quality\")\n    print(\"=\"*70)\n    avg_intra, avg_inter = compute_embedding_quality(triplet_model, test_loader)\n    separation_ratio = avg_inter / avg_intra if avg_intra > 0 else 0\n    print(f\"Average Intra-class Distance: {avg_intra:.4f}\")\n    print(f\"Average Inter-class Distance: {avg_inter:.4f}\")\n    print(f\"Separation Ratio (Inter/Intra): {separation_ratio:.4f}\")\n    \n    if separation_ratio < 2.5:\n        print(\"âš ï¸  Warning: Separation ratio is low. Consider training longer.\")\n    else:\n        print(\"âœ“ Good embedding separation achieved!\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"PHASE 2: Training Prediction Network\")\n    print(\"=\"*70)\n    \n    pred_optimizer = optim.SGD(pred_network.parameters(), lr=LEARNING_RATE_PRED, \n                               momentum=0.9, weight_decay=5e-4, nesterov=True)\n    pred_scheduler = optim.lr_scheduler.OneCycleLR(\n        pred_optimizer, max_lr=LEARNING_RATE_PRED, \n        epochs=NUM_EPOCHS_PRED, steps_per_epoch=len(train_loader),\n        pct_start=0.3, anneal_strategy='cos'\n    )\n    \n    best_test_acc = 0\n    best_epoch = 0\n    patience = 0\n    max_patience = 20\n    \n    for epoch in range(1, NUM_EPOCHS_PRED + 1):\n        train_loss, train_acc = train_prediction_network(\n            triplet_model, pred_network, pred_optimizer, train_loader, epoch\n        )\n        \n        # Step scheduler per batch\n        if epoch == 1:\n            for _ in range(len(train_loader)):\n                pred_scheduler.step()\n        \n        test_acc = test_classification(triplet_model, pred_network, test_loader)\n        \n        current_lr = pred_optimizer.param_groups[0]['lr']\n        print(f'Epoch {epoch:3d} | LR: {current_lr:.5f} | Train Loss: {train_loss:.4f} | '\n              f'Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%', end='')\n        \n        if test_acc > best_test_acc:\n            best_test_acc = test_acc\n            best_epoch = epoch\n            patience = 0\n            torch.save({\n                'epoch': epoch,\n                'triplet_model': triplet_model.state_dict(),\n                'pred_network': pred_network.state_dict(),\n                'test_acc': test_acc,\n                'optimizer': pred_optimizer.state_dict(),\n            }, 'best_optimized_model.pth')\n            print(f' âœ“ NEW BEST!')\n        else:\n            patience += 1\n            print()\n        \n        # Early stopping\n        if patience >= max_patience:\n            print(f\"\\nEarly stopping triggered at epoch {epoch} (patience={max_patience})\")\n            break\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"FINAL RESULTS\")\n    print(\"=\"*70)\n    print(f\"Best Test Accuracy: {best_test_acc:.2f}% (achieved at epoch {best_epoch})\")\n    print(f\"Embedding Separation Ratio: {separation_ratio:.2f}x\")\n    \n    if best_test_acc >= 85:\n        print(f\"\\nðŸŽ‰ EXCELLENT! Target exceeded ({best_test_acc:.2f}% >= 85%)\")\n    elif best_test_acc >= 80:\n        print(f\"\\nâœ“ SUCCESS! Target achieved ({best_test_acc:.2f}% >= 80%)\")\n    elif best_test_acc >= 75:\n        print(f\"\\nâœ“ GOOD! Close to target ({best_test_acc:.2f}%)\")\n    else:\n        print(f\"\\nCurrent: {best_test_acc:.2f}%, Target: 80.00%\")\n        print(f\"Gap to target: {80.00 - best_test_acc:.2f}%\")\n    \n    return triplet_model, pred_network, best_test_acc\n\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\"*70)\n    print(\"Optimized ResNet-18 Triplet Network with A-SCTL\")\n    print(\"Target: 80-85% Test Accuracy on CIFAR-10\")\n    print(\"=\"*70 + \"\\n\")\n    \n    triplet_model, pred_network, accuracy = main()\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"Training completed! Best model saved to 'best_optimized_model.pth'\")\n    print(\"=\"*70)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T09:58:30.649424Z","iopub.execute_input":"2025-12-15T09:58:30.650054Z","iopub.status.idle":"2025-12-15T13:51:36.710690Z","shell.execute_reply.started":"2025-12-15T09:58:30.650023Z","shell.execute_reply":"2025-12-15T13:51:36.709911Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n======================================================================\nOptimized ResNet-18 Triplet Network with A-SCTL\nTarget: 80-85% Test Accuracy on CIFAR-10\n======================================================================\n\nLoading CIFAR-10 dataset...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:02<00:00, 66.3MB/s] \n","output_type":"stream"},{"name":"stdout","text":"\nInitializing ResNet-18 Triplet Network with A-SCTL...\n\n======================================================================\nPHASE 1: Training Triplet Network with A-SCTL\n======================================================================\nEpoch 1 [0/391] Loss: 71.8246 | Intra: 239.4152 | Inter: 0.0000 | Î»: 0.700\nEpoch 1 [100/391] Loss: 8.9577 | Intra: 89.4097 | Inter: 0.0186 | Î»: 0.900\nEpoch 1 [200/391] Loss: 4.8185 | Intra: 47.8333 | Inter: 0.0391 | Î»: 0.900\nEpoch 1 [300/391] Loss: 4.6714 | Intra: 46.0021 | Inter: 0.0791 | Î»: 0.900\nEpoch  1 | Loss: 7.8550 | Intra: 76.3206 | Inter: 0.0502 | Î»: 0.8993\nEpoch 2 [0/391] Loss: 2.4008 | Intra: 23.4455 | Inter: 0.0625 | Î»: 0.900\nEpoch 2 [100/391] Loss: 2.1799 | Intra: 20.8377 | Inter: 0.1068 | Î»: 0.900\nEpoch 2 [200/391] Loss: 0.9128 | Intra: 8.3856 | Inter: 0.0825 | Î»: 0.900\nEpoch 2 [300/391] Loss: 0.3274 | Intra: 2.6171 | Inter: 0.0730 | Î»: 0.900\nEpoch  2 | Loss: 1.2099 | Intra: 11.4800 | Inter: 0.0687 | Î»: 0.9000\nEpoch 3 [0/391] Loss: 0.2109 | Intra: 1.4012 | Inter: 0.0786 | Î»: 0.900\nEpoch 3 [100/391] Loss: 0.1878 | Intra: 1.3275 | Inter: 0.0612 | Î»: 0.900\nEpoch 3 [200/391] Loss: 0.2127 | Intra: 1.2043 | Inter: 0.1026 | Î»: 0.900\nEpoch 3 [300/391] Loss: 0.1861 | Intra: 1.1361 | Inter: 0.0806 | Î»: 0.900\nEpoch  3 | Loss: 0.1804 | Intra: 1.2365 | Inter: 0.0630 | Î»: 0.9000\nEpoch 4 [0/391] Loss: 0.1738 | Intra: 1.1533 | Inter: 0.0650 | Î»: 0.900\nEpoch 4 [100/391] Loss: 0.1476 | Intra: 1.1616 | Inter: 0.0349 | Î»: 0.900\nEpoch 4 [200/391] Loss: 0.1857 | Intra: 1.0759 | Inter: 0.0868 | Î»: 0.900\nEpoch 4 [300/391] Loss: 0.1671 | Intra: 1.1530 | Inter: 0.0575 | Î»: 0.900\nEpoch  4 | Loss: 0.1642 | Intra: 1.1837 | Inter: 0.0509 | Î»: 0.9000\nEpoch 5 [0/391] Loss: 0.1818 | Intra: 1.1990 | Inter: 0.0688 | Î»: 0.900\nEpoch 5 [100/391] Loss: 0.1529 | Intra: 1.2530 | Inter: 0.0307 | Î»: 0.900\nEpoch 5 [200/391] Loss: 0.1570 | Intra: 1.1404 | Inter: 0.0477 | Î»: 0.900\nEpoch 5 [300/391] Loss: 0.1520 | Intra: 1.2120 | Inter: 0.0342 | Î»: 0.900\nEpoch  5 | Loss: 0.1579 | Intra: 1.1688 | Inter: 0.0456 | Î»: 0.9000\nEpoch 6 [0/391] Loss: 0.1622 | Intra: 1.1145 | Inter: 0.0564 | Î»: 0.900\nEpoch 6 [100/391] Loss: 0.1518 | Intra: 1.1488 | Inter: 0.0410 | Î»: 0.900\nEpoch 6 [200/391] Loss: 0.1544 | Intra: 1.1252 | Inter: 0.0465 | Î»: 0.900\nEpoch 6 [300/391] Loss: 0.1646 | Intra: 1.1883 | Inter: 0.0508 | Î»: 0.900\nEpoch  6 | Loss: 0.1538 | Intra: 1.1616 | Inter: 0.0419 | Î»: 0.9000\nEpoch 7 [0/391] Loss: 0.1492 | Intra: 1.1822 | Inter: 0.0344 | Î»: 0.900\nEpoch 7 [100/391] Loss: 0.1476 | Intra: 1.1481 | Inter: 0.0365 | Î»: 0.900\nEpoch 7 [200/391] Loss: 0.1504 | Intra: 1.1134 | Inter: 0.0434 | Î»: 0.900\nEpoch 7 [300/391] Loss: 0.1457 | Intra: 1.1681 | Inter: 0.0321 | Î»: 0.900\nEpoch  7 | Loss: 0.1477 | Intra: 1.1390 | Inter: 0.0376 | Î»: 0.9000\nEpoch 8 [0/391] Loss: 0.1406 | Intra: 1.1072 | Inter: 0.0332 | Î»: 0.900\nEpoch 8 [100/391] Loss: 0.1590 | Intra: 1.1582 | Inter: 0.0480 | Î»: 0.900\nEpoch 8 [200/391] Loss: 0.1418 | Intra: 1.1629 | Inter: 0.0284 | Î»: 0.900\nEpoch 8 [300/391] Loss: 0.1525 | Intra: 1.2581 | Inter: 0.0297 | Î»: 0.900\nEpoch  8 | Loss: 0.1458 | Intra: 1.1365 | Inter: 0.0357 | Î»: 0.9000\nEpoch 9 [0/391] Loss: 0.1534 | Intra: 1.1605 | Inter: 0.0415 | Î»: 0.900\nEpoch 9 [100/391] Loss: 0.1393 | Intra: 1.1078 | Inter: 0.0317 | Î»: 0.900\nEpoch 9 [200/391] Loss: 0.1555 | Intra: 1.1314 | Inter: 0.0471 | Î»: 0.900\nEpoch 9 [300/391] Loss: 0.1401 | Intra: 1.1434 | Inter: 0.0286 | Î»: 0.900\nEpoch  9 | Loss: 0.1440 | Intra: 1.1369 | Inter: 0.0337 | Î»: 0.9000\nEpoch 10 [0/391] Loss: 0.1386 | Intra: 1.1328 | Inter: 0.0281 | Î»: 0.900\nEpoch 10 [100/391] Loss: 0.1312 | Intra: 1.0927 | Inter: 0.0244 | Î»: 0.900\nEpoch 10 [200/391] Loss: 0.1257 | Intra: 1.0521 | Inter: 0.0228 | Î»: 0.900\nEpoch 10 [300/391] Loss: 0.1447 | Intra: 1.1250 | Inter: 0.0358 | Î»: 0.900\nEpoch 10 | Loss: 0.1411 | Intra: 1.1236 | Inter: 0.0319 | Î»: 0.9000\nUpdating embeddings for hard negative mining...\nâœ“ Embeddings updated (epoch 10)\nEpoch 11 [0/391] Loss: 0.1371 | Intra: 1.1246 | Inter: 0.0274 | Î»: 0.900\nEpoch 11 [100/391] Loss: 0.1423 | Intra: 1.1544 | Inter: 0.0298 | Î»: 0.900\nEpoch 11 [200/391] Loss: 0.1378 | Intra: 1.1615 | Inter: 0.0241 | Î»: 0.900\nEpoch 11 [300/391] Loss: 0.1422 | Intra: 1.1649 | Inter: 0.0285 | Î»: 0.900\nEpoch 11 | Loss: 0.1392 | Intra: 1.1099 | Inter: 0.0314 | Î»: 0.9000\nEpoch 12 [0/391] Loss: 0.1449 | Intra: 1.1052 | Inter: 0.0382 | Î»: 0.900\nEpoch 12 [100/391] Loss: 0.1301 | Intra: 1.1302 | Inter: 0.0190 | Î»: 0.900\nEpoch 12 [200/391] Loss: 0.1420 | Intra: 1.1101 | Inter: 0.0344 | Î»: 0.900\nEpoch 12 [300/391] Loss: 0.1286 | Intra: 1.0858 | Inter: 0.0223 | Î»: 0.900\nEpoch 12 | Loss: 0.1380 | Intra: 1.1023 | Inter: 0.0309 | Î»: 0.9000\nEpoch 13 [0/391] Loss: 0.1243 | Intra: 1.0649 | Inter: 0.0198 | Î»: 0.900\nEpoch 13 [100/391] Loss: 0.1406 | Intra: 1.0991 | Inter: 0.0341 | Î»: 0.900\nEpoch 13 [200/391] Loss: 0.1433 | Intra: 1.1010 | Inter: 0.0369 | Î»: 0.900\nEpoch 13 [300/391] Loss: 0.1393 | Intra: 1.0838 | Inter: 0.0343 | Î»: 0.900\nEpoch 13 | Loss: 0.1360 | Intra: 1.0883 | Inter: 0.0302 | Î»: 0.9000\nEpoch 14 [0/391] Loss: 0.1453 | Intra: 1.0732 | Inter: 0.0422 | Î»: 0.900\nEpoch 14 [100/391] Loss: 0.1351 | Intra: 1.0956 | Inter: 0.0284 | Î»: 0.900\nEpoch 14 [200/391] Loss: 0.1387 | Intra: 1.1021 | Inter: 0.0317 | Î»: 0.900\nEpoch 14 [300/391] Loss: 0.1380 | Intra: 1.1197 | Inter: 0.0289 | Î»: 0.900\nEpoch 14 | Loss: 0.1355 | Intra: 1.0858 | Inter: 0.0299 | Î»: 0.9000\nEpoch 15 [0/391] Loss: 0.1422 | Intra: 1.1409 | Inter: 0.0312 | Î»: 0.900\nEpoch 15 [100/391] Loss: 0.1274 | Intra: 1.0357 | Inter: 0.0265 | Î»: 0.900\nEpoch 15 [200/391] Loss: 0.1305 | Intra: 1.0599 | Inter: 0.0272 | Î»: 0.900\nEpoch 15 [300/391] Loss: 0.1348 | Intra: 1.0180 | Inter: 0.0366 | Î»: 0.900\nEpoch 15 | Loss: 0.1322 | Intra: 1.0574 | Inter: 0.0295 | Î»: 0.9000\nEpoch 16 [0/391] Loss: 0.1286 | Intra: 1.0925 | Inter: 0.0215 | Î»: 0.900\nEpoch 16 [100/391] Loss: 0.1294 | Intra: 1.0440 | Inter: 0.0278 | Î»: 0.900\nEpoch 16 [200/391] Loss: 0.1436 | Intra: 1.0882 | Inter: 0.0386 | Î»: 0.900\nEpoch 16 [300/391] Loss: 0.1210 | Intra: 1.0021 | Inter: 0.0231 | Î»: 0.900\nEpoch 16 | Loss: 0.1315 | Intra: 1.0487 | Inter: 0.0296 | Î»: 0.9000\nEpoch 17 [0/391] Loss: 0.1226 | Intra: 0.9983 | Inter: 0.0253 | Î»: 0.900\nEpoch 17 [100/391] Loss: 0.1347 | Intra: 1.0791 | Inter: 0.0298 | Î»: 0.900\nEpoch 17 [200/391] Loss: 0.1316 | Intra: 1.0392 | Inter: 0.0307 | Î»: 0.900\nEpoch 17 [300/391] Loss: 0.1421 | Intra: 1.0485 | Inter: 0.0413 | Î»: 0.900\nEpoch 17 | Loss: 0.1291 | Intra: 1.0292 | Inter: 0.0291 | Î»: 0.9000\nEpoch 18 [0/391] Loss: 0.1303 | Intra: 1.0512 | Inter: 0.0279 | Î»: 0.900\nEpoch 18 [100/391] Loss: 0.1305 | Intra: 1.0508 | Inter: 0.0283 | Î»: 0.900\nEpoch 18 [200/391] Loss: 0.1353 | Intra: 1.0310 | Inter: 0.0358 | Î»: 0.900\nEpoch 18 [300/391] Loss: 0.1471 | Intra: 1.1071 | Inter: 0.0404 | Î»: 0.900\nEpoch 18 | Loss: 0.1271 | Intra: 1.0128 | Inter: 0.0287 | Î»: 0.9000\nEpoch 19 [0/391] Loss: 0.1232 | Intra: 1.0503 | Inter: 0.0202 | Î»: 0.900\nEpoch 19 [100/391] Loss: 0.1288 | Intra: 0.9946 | Inter: 0.0326 | Î»: 0.900\nEpoch 19 [200/391] Loss: 0.1434 | Intra: 1.0720 | Inter: 0.0402 | Î»: 0.900\nEpoch 19 [300/391] Loss: 0.1344 | Intra: 1.0334 | Inter: 0.0346 | Î»: 0.900\nEpoch 19 | Loss: 0.1249 | Intra: 0.9970 | Inter: 0.0280 | Î»: 0.9000\nEpoch 20 [0/391] Loss: 0.1201 | Intra: 0.9562 | Inter: 0.0272 | Î»: 0.900\nEpoch 20 [100/391] Loss: 0.1237 | Intra: 1.0187 | Inter: 0.0242 | Î»: 0.900\nEpoch 20 [200/391] Loss: 0.1172 | Intra: 0.9661 | Inter: 0.0229 | Î»: 0.900\nEpoch 20 [300/391] Loss: 0.1273 | Intra: 0.9737 | Inter: 0.0332 | Î»: 0.900\nEpoch 20 | Loss: 0.1232 | Intra: 0.9792 | Inter: 0.0281 | Î»: 0.9000\nUpdating embeddings for hard negative mining...\nâœ“ Embeddings updated (epoch 20)\nEpoch 21 [0/391] Loss: 0.1291 | Intra: 0.9446 | Inter: 0.0385 | Î»: 0.900\nEpoch 21 [100/391] Loss: 0.1222 | Intra: 1.0140 | Inter: 0.0231 | Î»: 0.900\nEpoch 21 [200/391] Loss: 0.1122 | Intra: 0.9231 | Inter: 0.0221 | Î»: 0.900\nEpoch 21 [300/391] Loss: 0.1184 | Intra: 0.9112 | Inter: 0.0303 | Î»: 0.900\nEpoch 21 | Loss: 0.1215 | Intra: 0.9625 | Inter: 0.0280 | Î»: 0.9000\nEpoch 22 [0/391] Loss: 0.1286 | Intra: 0.9901 | Inter: 0.0328 | Î»: 0.900\nEpoch 22 [100/391] Loss: 0.1125 | Intra: 0.9047 | Inter: 0.0245 | Î»: 0.900\nEpoch 22 [200/391] Loss: 0.1134 | Intra: 0.9108 | Inter: 0.0248 | Î»: 0.900\nEpoch 22 [300/391] Loss: 0.1251 | Intra: 0.9503 | Inter: 0.0334 | Î»: 0.900\nEpoch 22 | Loss: 0.1197 | Intra: 0.9476 | Inter: 0.0277 | Î»: 0.9000\nEpoch 23 [0/391] Loss: 0.1173 | Intra: 0.8921 | Inter: 0.0313 | Î»: 0.900\nEpoch 23 [100/391] Loss: 0.1202 | Intra: 0.9182 | Inter: 0.0315 | Î»: 0.900\nEpoch 23 [200/391] Loss: 0.0989 | Intra: 0.8236 | Inter: 0.0184 | Î»: 0.900\nEpoch 23 [300/391] Loss: 0.1154 | Intra: 0.9934 | Inter: 0.0179 | Î»: 0.900\nEpoch 23 | Loss: 0.1178 | Intra: 0.9274 | Inter: 0.0278 | Î»: 0.9000\nEpoch 24 [0/391] Loss: 0.1102 | Intra: 0.8809 | Inter: 0.0246 | Î»: 0.900\nEpoch 24 [100/391] Loss: 0.1170 | Intra: 0.9039 | Inter: 0.0295 | Î»: 0.900\nEpoch 24 [200/391] Loss: 0.1159 | Intra: 0.8870 | Inter: 0.0302 | Î»: 0.900\nEpoch 24 [300/391] Loss: 0.1147 | Intra: 0.8971 | Inter: 0.0277 | Î»: 0.900\nEpoch 24 | Loss: 0.1165 | Intra: 0.9113 | Inter: 0.0281 | Î»: 0.9000\nEpoch 25 [0/391] Loss: 0.1182 | Intra: 0.9274 | Inter: 0.0282 | Î»: 0.900\nEpoch 25 [100/391] Loss: 0.1184 | Intra: 0.8632 | Inter: 0.0356 | Î»: 0.900\nEpoch 25 [200/391] Loss: 0.1096 | Intra: 0.8955 | Inter: 0.0223 | Î»: 0.900\nEpoch 25 [300/391] Loss: 0.1033 | Intra: 0.8729 | Inter: 0.0178 | Î»: 0.900\nEpoch 25 | Loss: 0.1144 | Intra: 0.8982 | Inter: 0.0273 | Î»: 0.9000\nEpoch 26 [0/391] Loss: 0.1157 | Intra: 0.8591 | Inter: 0.0331 | Î»: 0.900\nEpoch 26 [100/391] Loss: 0.1069 | Intra: 0.8272 | Inter: 0.0268 | Î»: 0.900\nEpoch 26 [200/391] Loss: 0.1076 | Intra: 0.8609 | Inter: 0.0239 | Î»: 0.900\nEpoch 26 [300/391] Loss: 0.1215 | Intra: 0.8292 | Inter: 0.0429 | Î»: 0.900\nEpoch 26 | Loss: 0.1127 | Intra: 0.8811 | Inter: 0.0273 | Î»: 0.9000\nEpoch 27 [0/391] Loss: 0.1251 | Intra: 0.8628 | Inter: 0.0431 | Î»: 0.900\nEpoch 27 [100/391] Loss: 0.1022 | Intra: 0.9006 | Inter: 0.0135 | Î»: 0.900\nEpoch 27 [200/391] Loss: 0.1201 | Intra: 0.8961 | Inter: 0.0338 | Î»: 0.900\nEpoch 27 [300/391] Loss: 0.1292 | Intra: 0.8885 | Inter: 0.0449 | Î»: 0.900\nEpoch 27 | Loss: 0.1103 | Intra: 0.8595 | Inter: 0.0270 | Î»: 0.9000\nEpoch 28 [0/391] Loss: 0.1033 | Intra: 0.8371 | Inter: 0.0217 | Î»: 0.900\nEpoch 28 [100/391] Loss: 0.1020 | Intra: 0.7924 | Inter: 0.0253 | Î»: 0.900\nEpoch 28 [200/391] Loss: 0.1117 | Intra: 0.8577 | Inter: 0.0288 | Î»: 0.900\nEpoch 28 [300/391] Loss: 0.1151 | Intra: 0.8516 | Inter: 0.0332 | Î»: 0.900\nEpoch 28 | Loss: 0.1095 | Intra: 0.8482 | Inter: 0.0274 | Î»: 0.9000\nEpoch 29 [0/391] Loss: 0.1135 | Intra: 0.8779 | Inter: 0.0285 | Î»: 0.900\nEpoch 29 [100/391] Loss: 0.0976 | Intra: 0.8035 | Inter: 0.0192 | Î»: 0.900\nEpoch 29 [200/391] Loss: 0.0988 | Intra: 0.7671 | Inter: 0.0246 | Î»: 0.900\nEpoch 29 [300/391] Loss: 0.0928 | Intra: 0.8166 | Inter: 0.0124 | Î»: 0.900\nEpoch 29 | Loss: 0.1069 | Intra: 0.8280 | Inter: 0.0268 | Î»: 0.9000\nEpoch 30 [0/391] Loss: 0.1047 | Intra: 0.8599 | Inter: 0.0207 | Î»: 0.900\nEpoch 30 [100/391] Loss: 0.0952 | Intra: 0.7829 | Inter: 0.0188 | Î»: 0.900\nEpoch 30 [200/391] Loss: 0.1079 | Intra: 0.8035 | Inter: 0.0306 | Î»: 0.900\nEpoch 30 [300/391] Loss: 0.1081 | Intra: 0.8094 | Inter: 0.0301 | Î»: 0.900\nEpoch 30 | Loss: 0.1055 | Intra: 0.8144 | Inter: 0.0267 | Î»: 0.9000\nUpdating embeddings for hard negative mining...\nâœ“ Embeddings updated (epoch 30)\nEpoch 31 [0/391] Loss: 0.1122 | Intra: 0.8335 | Inter: 0.0320 | Î»: 0.900\nEpoch 31 [100/391] Loss: 0.0958 | Intra: 0.7872 | Inter: 0.0190 | Î»: 0.900\nEpoch 31 [200/391] Loss: 0.0956 | Intra: 0.8685 | Inter: 0.0097 | Î»: 0.900\nEpoch 31 [300/391] Loss: 0.1111 | Intra: 0.8092 | Inter: 0.0335 | Î»: 0.900\nEpoch 31 | Loss: 0.1038 | Intra: 0.7998 | Inter: 0.0264 | Î»: 0.9000\nEpoch 32 [0/391] Loss: 0.1128 | Intra: 0.7934 | Inter: 0.0372 | Î»: 0.900\nEpoch 32 [100/391] Loss: 0.0980 | Intra: 0.7738 | Inter: 0.0229 | Î»: 0.900\nEpoch 32 [200/391] Loss: 0.1035 | Intra: 0.8411 | Inter: 0.0215 | Î»: 0.900\nEpoch 32 [300/391] Loss: 0.0914 | Intra: 0.8118 | Inter: 0.0113 | Î»: 0.900\nEpoch 32 | Loss: 0.1022 | Intra: 0.7917 | Inter: 0.0256 | Î»: 0.9000\nEpoch 33 [0/391] Loss: 0.0945 | Intra: 0.7494 | Inter: 0.0217 | Î»: 0.900\nEpoch 33 [100/391] Loss: 0.1103 | Intra: 0.7751 | Inter: 0.0365 | Î»: 0.900\nEpoch 33 [200/391] Loss: 0.0930 | Intra: 0.7364 | Inter: 0.0216 | Î»: 0.900\nEpoch 33 [300/391] Loss: 0.0955 | Intra: 0.6782 | Inter: 0.0307 | Î»: 0.900\nEpoch 33 | Loss: 0.1003 | Intra: 0.7698 | Inter: 0.0259 | Î»: 0.9000\nEpoch 34 [0/391] Loss: 0.1059 | Intra: 0.7853 | Inter: 0.0304 | Î»: 0.900\nEpoch 34 [100/391] Loss: 0.0973 | Intra: 0.7574 | Inter: 0.0239 | Î»: 0.900\nEpoch 34 [200/391] Loss: 0.0859 | Intra: 0.7129 | Inter: 0.0163 | Î»: 0.900\nEpoch 34 [300/391] Loss: 0.1017 | Intra: 0.7759 | Inter: 0.0267 | Î»: 0.900\nEpoch 34 | Loss: 0.0993 | Intra: 0.7568 | Inter: 0.0262 | Î»: 0.9000\nEpoch 35 [0/391] Loss: 0.1036 | Intra: 0.7444 | Inter: 0.0324 | Î»: 0.900\nEpoch 35 [100/391] Loss: 0.1160 | Intra: 0.7719 | Inter: 0.0431 | Î»: 0.900\nEpoch 35 [200/391] Loss: 0.0934 | Intra: 0.7425 | Inter: 0.0212 | Î»: 0.900\nEpoch 35 [300/391] Loss: 0.0820 | Intra: 0.7291 | Inter: 0.0101 | Î»: 0.900\nEpoch 35 | Loss: 0.0982 | Intra: 0.7450 | Inter: 0.0264 | Î»: 0.9000\nEpoch 36 [0/391] Loss: 0.0982 | Intra: 0.7892 | Inter: 0.0214 | Î»: 0.900\nEpoch 36 [100/391] Loss: 0.0954 | Intra: 0.7577 | Inter: 0.0218 | Î»: 0.900\nEpoch 36 [200/391] Loss: 0.0897 | Intra: 0.8065 | Inter: 0.0100 | Î»: 0.900\nEpoch 36 [300/391] Loss: 0.0876 | Intra: 0.6748 | Inter: 0.0223 | Î»: 0.900\nEpoch 36 | Loss: 0.0971 | Intra: 0.7345 | Inter: 0.0262 | Î»: 0.9000\nEpoch 37 [0/391] Loss: 0.0936 | Intra: 0.7077 | Inter: 0.0253 | Î»: 0.900\nEpoch 37 [100/391] Loss: 0.0896 | Intra: 0.7065 | Inter: 0.0210 | Î»: 0.900\nEpoch 37 [200/391] Loss: 0.0912 | Intra: 0.7372 | Inter: 0.0194 | Î»: 0.900\nEpoch 37 [300/391] Loss: 0.0942 | Intra: 0.7602 | Inter: 0.0202 | Î»: 0.900\nEpoch 37 | Loss: 0.0960 | Intra: 0.7258 | Inter: 0.0260 | Î»: 0.9000\nEpoch 38 [0/391] Loss: 0.0999 | Intra: 0.7481 | Inter: 0.0279 | Î»: 0.900\nEpoch 38 [100/391] Loss: 0.0888 | Intra: 0.6695 | Inter: 0.0243 | Î»: 0.900\nEpoch 38 [200/391] Loss: 0.1121 | Intra: 0.7347 | Inter: 0.0429 | Î»: 0.900\nEpoch 38 [300/391] Loss: 0.0767 | Intra: 0.6847 | Inter: 0.0092 | Î»: 0.900\nEpoch 38 | Loss: 0.0936 | Intra: 0.7125 | Inter: 0.0249 | Î»: 0.9000\nEpoch 39 [0/391] Loss: 0.0803 | Intra: 0.6418 | Inter: 0.0179 | Î»: 0.900\nEpoch 39 [100/391] Loss: 0.0932 | Intra: 0.7093 | Inter: 0.0247 | Î»: 0.900\nEpoch 39 [200/391] Loss: 0.0991 | Intra: 0.7731 | Inter: 0.0242 | Î»: 0.900\nEpoch 39 [300/391] Loss: 0.1055 | Intra: 0.7141 | Inter: 0.0379 | Î»: 0.900\nEpoch 39 | Loss: 0.0916 | Intra: 0.6958 | Inter: 0.0244 | Î»: 0.9000\nEpoch 40 [0/391] Loss: 0.0771 | Intra: 0.6137 | Inter: 0.0175 | Î»: 0.900\nEpoch 40 [100/391] Loss: 0.0850 | Intra: 0.7236 | Inter: 0.0140 | Î»: 0.900\nEpoch 40 [200/391] Loss: 0.0896 | Intra: 0.7042 | Inter: 0.0213 | Î»: 0.900\nEpoch 40 [300/391] Loss: 0.0815 | Intra: 0.6641 | Inter: 0.0168 | Î»: 0.900\nEpoch 40 | Loss: 0.0909 | Intra: 0.6848 | Inter: 0.0249 | Î»: 0.9000\nUpdating embeddings for hard negative mining...\nâœ“ Embeddings updated (epoch 40)\nEpoch 41 [0/391] Loss: 0.1015 | Intra: 0.7116 | Inter: 0.0337 | Î»: 0.900\nEpoch 41 [100/391] Loss: 0.0867 | Intra: 0.6882 | Inter: 0.0198 | Î»: 0.900\nEpoch 41 [200/391] Loss: 0.0894 | Intra: 0.6563 | Inter: 0.0264 | Î»: 0.900\nEpoch 41 [300/391] Loss: 0.0896 | Intra: 0.6539 | Inter: 0.0269 | Î»: 0.900\nEpoch 41 | Loss: 0.0904 | Intra: 0.6799 | Inter: 0.0248 | Î»: 0.9000\nEpoch 42 [0/391] Loss: 0.0824 | Intra: 0.6338 | Inter: 0.0211 | Î»: 0.900\nEpoch 42 [100/391] Loss: 0.0798 | Intra: 0.6309 | Inter: 0.0186 | Î»: 0.900\nEpoch 42 [200/391] Loss: 0.0948 | Intra: 0.7116 | Inter: 0.0262 | Î»: 0.900\nEpoch 42 [300/391] Loss: 0.0962 | Intra: 0.6082 | Inter: 0.0393 | Î»: 0.900\nEpoch 42 | Loss: 0.0886 | Intra: 0.6623 | Inter: 0.0249 | Î»: 0.9000\nEpoch 43 [0/391] Loss: 0.1059 | Intra: 0.6039 | Inter: 0.0505 | Î»: 0.900\nEpoch 43 [100/391] Loss: 0.0949 | Intra: 0.5900 | Inter: 0.0399 | Î»: 0.900\nEpoch 43 [200/391] Loss: 0.1070 | Intra: 0.6801 | Inter: 0.0433 | Î»: 0.900\nEpoch 43 [300/391] Loss: 0.0777 | Intra: 0.6846 | Inter: 0.0103 | Î»: 0.900\nEpoch 43 | Loss: 0.0874 | Intra: 0.6526 | Inter: 0.0246 | Î»: 0.9000\nEpoch 44 [0/391] Loss: 0.0827 | Intra: 0.7229 | Inter: 0.0116 | Î»: 0.900\nEpoch 44 [100/391] Loss: 0.0804 | Intra: 0.6314 | Inter: 0.0191 | Î»: 0.900\nEpoch 44 [200/391] Loss: 0.0902 | Intra: 0.6331 | Inter: 0.0299 | Î»: 0.900\nEpoch 44 [300/391] Loss: 0.0831 | Intra: 0.6659 | Inter: 0.0183 | Î»: 0.900\nEpoch 44 | Loss: 0.0865 | Intra: 0.6496 | Inter: 0.0239 | Î»: 0.9000\nEpoch 45 [0/391] Loss: 0.0755 | Intra: 0.5956 | Inter: 0.0177 | Î»: 0.900\nEpoch 45 [100/391] Loss: 0.0826 | Intra: 0.5569 | Inter: 0.0299 | Î»: 0.900\nEpoch 45 [200/391] Loss: 0.0834 | Intra: 0.6284 | Inter: 0.0228 | Î»: 0.900\nEpoch 45 [300/391] Loss: 0.0805 | Intra: 0.5268 | Inter: 0.0309 | Î»: 0.900\nEpoch 45 | Loss: 0.0844 | Intra: 0.6311 | Inter: 0.0237 | Î»: 0.9000\nEpoch 46 [0/391] Loss: 0.0708 | Intra: 0.5674 | Inter: 0.0156 | Î»: 0.900\nEpoch 46 [100/391] Loss: 0.0919 | Intra: 0.6706 | Inter: 0.0276 | Î»: 0.900\nEpoch 46 [200/391] Loss: 0.0985 | Intra: 0.6473 | Inter: 0.0376 | Î»: 0.900\nEpoch 46 [300/391] Loss: 0.0797 | Intra: 0.6474 | Inter: 0.0166 | Î»: 0.900\nEpoch 46 | Loss: 0.0837 | Intra: 0.6212 | Inter: 0.0240 | Î»: 0.9000\nEpoch 47 [0/391] Loss: 0.0765 | Intra: 0.6277 | Inter: 0.0153 | Î»: 0.900\nEpoch 47 [100/391] Loss: 0.0746 | Intra: 0.5713 | Inter: 0.0194 | Î»: 0.900\nEpoch 47 [200/391] Loss: 0.0826 | Intra: 0.6634 | Inter: 0.0181 | Î»: 0.900\nEpoch 47 [300/391] Loss: 0.0821 | Intra: 0.6794 | Inter: 0.0157 | Î»: 0.900\nEpoch 47 | Loss: 0.0819 | Intra: 0.6060 | Inter: 0.0237 | Î»: 0.9000\nEpoch 48 [0/391] Loss: 0.0817 | Intra: 0.6561 | Inter: 0.0179 | Î»: 0.900\nEpoch 48 [100/391] Loss: 0.0784 | Intra: 0.6937 | Inter: 0.0100 | Î»: 0.900\nEpoch 48 [200/391] Loss: 0.0767 | Intra: 0.6153 | Inter: 0.0168 | Î»: 0.900\nEpoch 48 [300/391] Loss: 0.0663 | Intra: 0.5590 | Inter: 0.0115 | Î»: 0.900\nEpoch 48 | Loss: 0.0815 | Intra: 0.6001 | Inter: 0.0238 | Î»: 0.9000\nEpoch 49 [0/391] Loss: 0.0754 | Intra: 0.5940 | Inter: 0.0177 | Î»: 0.900\nEpoch 49 [100/391] Loss: 0.0633 | Intra: 0.5793 | Inter: 0.0060 | Î»: 0.900\nEpoch 49 [200/391] Loss: 0.0765 | Intra: 0.6179 | Inter: 0.0164 | Î»: 0.900\nEpoch 49 [300/391] Loss: 0.0695 | Intra: 0.5559 | Inter: 0.0154 | Î»: 0.900\nEpoch 49 | Loss: 0.0802 | Intra: 0.5906 | Inter: 0.0235 | Î»: 0.9000\nEpoch 50 [0/391] Loss: 0.0890 | Intra: 0.5495 | Inter: 0.0379 | Î»: 0.900\nEpoch 50 [100/391] Loss: 0.0826 | Intra: 0.5396 | Inter: 0.0318 | Î»: 0.900\nEpoch 50 [200/391] Loss: 0.0838 | Intra: 0.6386 | Inter: 0.0221 | Î»: 0.900\nEpoch 50 [300/391] Loss: 0.0635 | Intra: 0.5492 | Inter: 0.0095 | Î»: 0.900\nEpoch 50 | Loss: 0.0791 | Intra: 0.5795 | Inter: 0.0235 | Î»: 0.9000\nUpdating embeddings for hard negative mining...\nâœ“ Embeddings updated (epoch 50)\nEpoch 51 [0/391] Loss: 0.0845 | Intra: 0.6689 | Inter: 0.0196 | Î»: 0.900\nEpoch 51 [100/391] Loss: 0.0820 | Intra: 0.5642 | Inter: 0.0284 | Î»: 0.900\nEpoch 51 [200/391] Loss: 0.0825 | Intra: 0.5627 | Inter: 0.0291 | Î»: 0.900\nEpoch 51 [300/391] Loss: 0.0757 | Intra: 0.5532 | Inter: 0.0227 | Î»: 0.900\nEpoch 51 | Loss: 0.0789 | Intra: 0.5810 | Inter: 0.0231 | Î»: 0.9000\nEpoch 52 [0/391] Loss: 0.0746 | Intra: 0.5291 | Inter: 0.0241 | Î»: 0.900\nEpoch 52 [100/391] Loss: 0.0803 | Intra: 0.6698 | Inter: 0.0148 | Î»: 0.900\nEpoch 52 [200/391] Loss: 0.0752 | Intra: 0.6060 | Inter: 0.0162 | Î»: 0.900\nEpoch 52 [300/391] Loss: 0.1010 | Intra: 0.5311 | Inter: 0.0532 | Î»: 0.900\nEpoch 52 | Loss: 0.0779 | Intra: 0.5670 | Inter: 0.0236 | Î»: 0.9000\nEpoch 53 [0/391] Loss: 0.0792 | Intra: 0.5236 | Inter: 0.0298 | Î»: 0.900\nEpoch 53 [100/391] Loss: 0.0832 | Intra: 0.5028 | Inter: 0.0366 | Î»: 0.900\nEpoch 53 [200/391] Loss: 0.0738 | Intra: 0.5810 | Inter: 0.0175 | Î»: 0.900\nEpoch 53 [300/391] Loss: 0.0747 | Intra: 0.6155 | Inter: 0.0146 | Î»: 0.900\nEpoch 53 | Loss: 0.0760 | Intra: 0.5562 | Inter: 0.0226 | Î»: 0.9000\nEpoch 54 [0/391] Loss: 0.0721 | Intra: 0.5632 | Inter: 0.0175 | Î»: 0.900\nEpoch 54 [100/391] Loss: 0.0815 | Intra: 0.6022 | Inter: 0.0236 | Î»: 0.900\nEpoch 54 [200/391] Loss: 0.0653 | Intra: 0.5349 | Inter: 0.0132 | Î»: 0.900\nEpoch 54 [300/391] Loss: 0.0787 | Intra: 0.5199 | Inter: 0.0297 | Î»: 0.900\nEpoch 54 | Loss: 0.0755 | Intra: 0.5484 | Inter: 0.0229 | Î»: 0.9000\nEpoch 55 [0/391] Loss: 0.0954 | Intra: 0.6085 | Inter: 0.0384 | Î»: 0.900\nEpoch 55 [100/391] Loss: 0.0711 | Intra: 0.5642 | Inter: 0.0163 | Î»: 0.900\nEpoch 55 [200/391] Loss: 0.0629 | Intra: 0.5009 | Inter: 0.0143 | Î»: 0.900\nEpoch 55 [300/391] Loss: 0.0612 | Intra: 0.5180 | Inter: 0.0105 | Î»: 0.900\nEpoch 55 | Loss: 0.0743 | Intra: 0.5398 | Inter: 0.0225 | Î»: 0.9000\nEpoch 56 [0/391] Loss: 0.0971 | Intra: 0.6263 | Inter: 0.0384 | Î»: 0.900\nEpoch 56 [100/391] Loss: 0.0823 | Intra: 0.5117 | Inter: 0.0345 | Î»: 0.900\nEpoch 56 [200/391] Loss: 0.0758 | Intra: 0.5351 | Inter: 0.0247 | Î»: 0.900\nEpoch 56 [300/391] Loss: 0.0649 | Intra: 0.4430 | Inter: 0.0228 | Î»: 0.900\nEpoch 56 | Loss: 0.0729 | Intra: 0.5251 | Inter: 0.0227 | Î»: 0.9000\nEpoch 57 [0/391] Loss: 0.0657 | Intra: 0.4436 | Inter: 0.0237 | Î»: 0.900\nEpoch 57 [100/391] Loss: 0.0619 | Intra: 0.4837 | Inter: 0.0151 | Î»: 0.900\nEpoch 57 [200/391] Loss: 0.0773 | Intra: 0.5530 | Inter: 0.0244 | Î»: 0.900\nEpoch 57 [300/391] Loss: 0.0618 | Intra: 0.4959 | Inter: 0.0136 | Î»: 0.900\nEpoch 57 | Loss: 0.0728 | Intra: 0.5171 | Inter: 0.0235 | Î»: 0.9000\nEpoch 58 [0/391] Loss: 0.0787 | Intra: 0.5288 | Inter: 0.0287 | Î»: 0.900\nEpoch 58 [100/391] Loss: 0.0845 | Intra: 0.6304 | Inter: 0.0238 | Î»: 0.900\nEpoch 58 [200/391] Loss: 0.0632 | Intra: 0.4659 | Inter: 0.0185 | Î»: 0.900\nEpoch 58 [300/391] Loss: 0.0834 | Intra: 0.5648 | Inter: 0.0300 | Î»: 0.900\nEpoch 58 | Loss: 0.0725 | Intra: 0.5213 | Inter: 0.0226 | Î»: 0.9000\nEpoch 59 [0/391] Loss: 0.0827 | Intra: 0.5619 | Inter: 0.0294 | Î»: 0.900\nEpoch 59 [100/391] Loss: 0.0708 | Intra: 0.5123 | Inter: 0.0218 | Î»: 0.900\nEpoch 59 [200/391] Loss: 0.0826 | Intra: 0.4947 | Inter: 0.0368 | Î»: 0.900\nEpoch 59 [300/391] Loss: 0.0777 | Intra: 0.5229 | Inter: 0.0283 | Î»: 0.900\nEpoch 59 | Loss: 0.0715 | Intra: 0.5159 | Inter: 0.0221 | Î»: 0.9000\nEpoch 60 [0/391] Loss: 0.0722 | Intra: 0.5616 | Inter: 0.0178 | Î»: 0.900\nEpoch 60 [100/391] Loss: 0.0574 | Intra: 0.4729 | Inter: 0.0112 | Î»: 0.900\nEpoch 60 [200/391] Loss: 0.0701 | Intra: 0.4881 | Inter: 0.0237 | Î»: 0.900\nEpoch 60 [300/391] Loss: 0.0773 | Intra: 0.5154 | Inter: 0.0286 | Î»: 0.900\nEpoch 60 | Loss: 0.0702 | Intra: 0.5080 | Inter: 0.0215 | Î»: 0.9000\nUpdating embeddings for hard negative mining...\nâœ“ Embeddings updated (epoch 60)\nEpoch 61 [0/391] Loss: 0.0842 | Intra: 0.5116 | Inter: 0.0367 | Î»: 0.900\nEpoch 61 [100/391] Loss: 0.0595 | Intra: 0.5363 | Inter: 0.0066 | Î»: 0.900\nEpoch 61 [200/391] Loss: 0.0654 | Intra: 0.4826 | Inter: 0.0190 | Î»: 0.900\nEpoch 61 [300/391] Loss: 0.0641 | Intra: 0.4983 | Inter: 0.0159 | Î»: 0.900\nEpoch 61 | Loss: 0.0696 | Intra: 0.4988 | Inter: 0.0219 | Î»: 0.9000\nEpoch 62 [0/391] Loss: 0.0658 | Intra: 0.5026 | Inter: 0.0173 | Î»: 0.900\nEpoch 62 [100/391] Loss: 0.0727 | Intra: 0.5343 | Inter: 0.0214 | Î»: 0.900\nEpoch 62 [200/391] Loss: 0.0889 | Intra: 0.4561 | Inter: 0.0481 | Î»: 0.900\nEpoch 62 [300/391] Loss: 0.0623 | Intra: 0.5000 | Inter: 0.0137 | Î»: 0.900\nEpoch 62 | Loss: 0.0684 | Intra: 0.4950 | Inter: 0.0210 | Î»: 0.9000\nEpoch 63 [0/391] Loss: 0.0633 | Intra: 0.4976 | Inter: 0.0150 | Î»: 0.900\nEpoch 63 [100/391] Loss: 0.0690 | Intra: 0.4677 | Inter: 0.0247 | Î»: 0.900\nEpoch 63 [200/391] Loss: 0.0716 | Intra: 0.4871 | Inter: 0.0254 | Î»: 0.900\nEpoch 63 [300/391] Loss: 0.0678 | Intra: 0.4624 | Inter: 0.0240 | Î»: 0.900\nEpoch 63 | Loss: 0.0687 | Intra: 0.4915 | Inter: 0.0217 | Î»: 0.9000\nEpoch 64 [0/391] Loss: 0.0608 | Intra: 0.4835 | Inter: 0.0138 | Î»: 0.900\nEpoch 64 [100/391] Loss: 0.0687 | Intra: 0.4990 | Inter: 0.0209 | Î»: 0.900\nEpoch 64 [200/391] Loss: 0.0696 | Intra: 0.4853 | Inter: 0.0234 | Î»: 0.900\nEpoch 64 [300/391] Loss: 0.0781 | Intra: 0.5790 | Inter: 0.0224 | Î»: 0.900\nEpoch 64 | Loss: 0.0678 | Intra: 0.4852 | Inter: 0.0215 | Î»: 0.9000\nEpoch 65 [0/391] Loss: 0.0691 | Intra: 0.4816 | Inter: 0.0232 | Î»: 0.900\nEpoch 65 [100/391] Loss: 0.0765 | Intra: 0.5025 | Inter: 0.0292 | Î»: 0.900\nEpoch 65 [200/391] Loss: 0.0644 | Intra: 0.4835 | Inter: 0.0178 | Î»: 0.900\nEpoch 65 [300/391] Loss: 0.0638 | Intra: 0.4583 | Inter: 0.0199 | Î»: 0.900\nEpoch 65 | Loss: 0.0665 | Intra: 0.4777 | Inter: 0.0208 | Î»: 0.9000\nEpoch 66 [0/391] Loss: 0.0558 | Intra: 0.4064 | Inter: 0.0168 | Î»: 0.900\nEpoch 66 [100/391] Loss: 0.0552 | Intra: 0.4629 | Inter: 0.0099 | Î»: 0.900\nEpoch 66 [200/391] Loss: 0.0537 | Intra: 0.4249 | Inter: 0.0124 | Î»: 0.900\nEpoch 66 [300/391] Loss: 0.0655 | Intra: 0.5126 | Inter: 0.0158 | Î»: 0.900\nEpoch 66 | Loss: 0.0663 | Intra: 0.4762 | Inter: 0.0208 | Î»: 0.9000\nEpoch 67 [0/391] Loss: 0.0726 | Intra: 0.5225 | Inter: 0.0226 | Î»: 0.900\nEpoch 67 [100/391] Loss: 0.0621 | Intra: 0.4611 | Inter: 0.0177 | Î»: 0.900\nEpoch 67 [200/391] Loss: 0.0630 | Intra: 0.4504 | Inter: 0.0199 | Î»: 0.900\nEpoch 67 [300/391] Loss: 0.0667 | Intra: 0.4907 | Inter: 0.0195 | Î»: 0.900\nEpoch 67 | Loss: 0.0656 | Intra: 0.4723 | Inter: 0.0204 | Î»: 0.9000\nEpoch 68 [0/391] Loss: 0.0763 | Intra: 0.4846 | Inter: 0.0309 | Î»: 0.900\nEpoch 68 [100/391] Loss: 0.0704 | Intra: 0.4853 | Inter: 0.0243 | Î»: 0.900\nEpoch 68 [200/391] Loss: 0.0580 | Intra: 0.4214 | Inter: 0.0176 | Î»: 0.900\nEpoch 68 [300/391] Loss: 0.0840 | Intra: 0.4703 | Inter: 0.0410 | Î»: 0.900\nEpoch 68 | Loss: 0.0661 | Intra: 0.4712 | Inter: 0.0210 | Î»: 0.9000\nEpoch 69 [0/391] Loss: 0.0548 | Intra: 0.4422 | Inter: 0.0118 | Î»: 0.900\nEpoch 69 [100/391] Loss: 0.0576 | Intra: 0.4253 | Inter: 0.0168 | Î»: 0.900\nEpoch 69 [200/391] Loss: 0.0621 | Intra: 0.4860 | Inter: 0.0150 | Î»: 0.900\nEpoch 69 [300/391] Loss: 0.0674 | Intra: 0.4784 | Inter: 0.0218 | Î»: 0.900\nEpoch 69 | Loss: 0.0658 | Intra: 0.4695 | Inter: 0.0210 | Î»: 0.9000\nEpoch 70 [0/391] Loss: 0.0529 | Intra: 0.3988 | Inter: 0.0145 | Î»: 0.900\nEpoch 70 [100/391] Loss: 0.0618 | Intra: 0.4539 | Inter: 0.0182 | Î»: 0.900\nEpoch 70 [200/391] Loss: 0.0683 | Intra: 0.5039 | Inter: 0.0198 | Î»: 0.900\nEpoch 70 [300/391] Loss: 0.0689 | Intra: 0.4786 | Inter: 0.0234 | Î»: 0.900\nEpoch 70 | Loss: 0.0654 | Intra: 0.4672 | Inter: 0.0208 | Î»: 0.9000\nUpdating embeddings for hard negative mining...\nâœ“ Embeddings updated (epoch 70)\nEpoch 71 [0/391] Loss: 0.0670 | Intra: 0.5413 | Inter: 0.0143 | Î»: 0.900\nEpoch 71 [100/391] Loss: 0.0814 | Intra: 0.5155 | Inter: 0.0332 | Î»: 0.900\nEpoch 71 [200/391] Loss: 0.0663 | Intra: 0.4914 | Inter: 0.0191 | Î»: 0.900\nEpoch 71 [300/391] Loss: 0.0642 | Intra: 0.4747 | Inter: 0.0186 | Î»: 0.900\nEpoch 71 | Loss: 0.0657 | Intra: 0.4649 | Inter: 0.0214 | Î»: 0.9000\nEpoch 72 [0/391] Loss: 0.0579 | Intra: 0.4428 | Inter: 0.0151 | Î»: 0.900\nEpoch 72 [100/391] Loss: 0.0725 | Intra: 0.4609 | Inter: 0.0294 | Î»: 0.900\nEpoch 72 [200/391] Loss: 0.0495 | Intra: 0.4259 | Inter: 0.0077 | Î»: 0.900\nEpoch 72 [300/391] Loss: 0.0603 | Intra: 0.4355 | Inter: 0.0186 | Î»: 0.900\nEpoch 72 | Loss: 0.0649 | Intra: 0.4651 | Inter: 0.0205 | Î»: 0.9000\nEpoch 73 [0/391] Loss: 0.0646 | Intra: 0.4556 | Inter: 0.0212 | Î»: 0.900\nEpoch 73 [100/391] Loss: 0.0583 | Intra: 0.4814 | Inter: 0.0113 | Î»: 0.900\nEpoch 73 [200/391] Loss: 0.0678 | Intra: 0.5170 | Inter: 0.0179 | Î»: 0.900\nEpoch 73 [300/391] Loss: 0.0674 | Intra: 0.4708 | Inter: 0.0225 | Î»: 0.900\nEpoch 73 | Loss: 0.0643 | Intra: 0.4591 | Inter: 0.0204 | Î»: 0.9000\nEpoch 74 [0/391] Loss: 0.0676 | Intra: 0.4888 | Inter: 0.0207 | Î»: 0.900\nEpoch 74 [100/391] Loss: 0.0654 | Intra: 0.5025 | Inter: 0.0168 | Î»: 0.900\nEpoch 74 [200/391] Loss: 0.0782 | Intra: 0.5382 | Inter: 0.0271 | Î»: 0.900\nEpoch 74 [300/391] Loss: 0.0681 | Intra: 0.4787 | Inter: 0.0225 | Î»: 0.900\nEpoch 74 | Loss: 0.0640 | Intra: 0.4611 | Inter: 0.0198 | Î»: 0.9000\nEpoch 75 [0/391] Loss: 0.0749 | Intra: 0.4765 | Inter: 0.0303 | Î»: 0.900\nEpoch 75 [100/391] Loss: 0.0662 | Intra: 0.4704 | Inter: 0.0213 | Î»: 0.900\nEpoch 75 [200/391] Loss: 0.0689 | Intra: 0.4698 | Inter: 0.0244 | Î»: 0.900\nEpoch 75 [300/391] Loss: 0.0529 | Intra: 0.3885 | Inter: 0.0156 | Î»: 0.900\nEpoch 75 | Loss: 0.0649 | Intra: 0.4596 | Inter: 0.0210 | Î»: 0.9000\nEpoch 76 [0/391] Loss: 0.0568 | Intra: 0.4009 | Inter: 0.0185 | Î»: 0.900\nEpoch 76 [100/391] Loss: 0.0756 | Intra: 0.5157 | Inter: 0.0267 | Î»: 0.900\nEpoch 76 [200/391] Loss: 0.0552 | Intra: 0.4469 | Inter: 0.0116 | Î»: 0.900\nEpoch 76 [300/391] Loss: 0.0688 | Intra: 0.4644 | Inter: 0.0249 | Î»: 0.900\nEpoch 76 | Loss: 0.0639 | Intra: 0.4576 | Inter: 0.0202 | Î»: 0.9000\nEpoch 77 [0/391] Loss: 0.0553 | Intra: 0.4723 | Inter: 0.0089 | Î»: 0.900\nEpoch 77 [100/391] Loss: 0.0627 | Intra: 0.4625 | Inter: 0.0183 | Î»: 0.900\nEpoch 77 [200/391] Loss: 0.0671 | Intra: 0.4459 | Inter: 0.0250 | Î»: 0.900\nEpoch 77 [300/391] Loss: 0.0711 | Intra: 0.4866 | Inter: 0.0249 | Î»: 0.900\nEpoch 77 | Loss: 0.0633 | Intra: 0.4550 | Inter: 0.0198 | Î»: 0.9000\nEpoch 78 [0/391] Loss: 0.0879 | Intra: 0.4672 | Inter: 0.0457 | Î»: 0.900\nEpoch 78 [100/391] Loss: 0.0550 | Intra: 0.4417 | Inter: 0.0120 | Î»: 0.900\nEpoch 78 [200/391] Loss: 0.0660 | Intra: 0.4596 | Inter: 0.0223 | Î»: 0.900\nEpoch 78 [300/391] Loss: 0.0652 | Intra: 0.4800 | Inter: 0.0191 | Î»: 0.900\nEpoch 78 | Loss: 0.0635 | Intra: 0.4560 | Inter: 0.0199 | Î»: 0.9000\nEpoch 79 [0/391] Loss: 0.0484 | Intra: 0.3966 | Inter: 0.0098 | Î»: 0.900\nEpoch 79 [100/391] Loss: 0.0466 | Intra: 0.4050 | Inter: 0.0067 | Î»: 0.900\nEpoch 79 [200/391] Loss: 0.0585 | Intra: 0.3726 | Inter: 0.0236 | Î»: 0.900\nEpoch 79 [300/391] Loss: 0.0596 | Intra: 0.4725 | Inter: 0.0137 | Î»: 0.900\nEpoch 79 | Loss: 0.0640 | Intra: 0.4559 | Inter: 0.0204 | Î»: 0.9000\nEpoch 80 [0/391] Loss: 0.0466 | Intra: 0.3425 | Inter: 0.0138 | Î»: 0.900\nEpoch 80 [100/391] Loss: 0.0737 | Intra: 0.4703 | Inter: 0.0296 | Î»: 0.900\nEpoch 80 [200/391] Loss: 0.0492 | Intra: 0.4585 | Inter: 0.0037 | Î»: 0.900\nEpoch 80 [300/391] Loss: 0.0761 | Intra: 0.5194 | Inter: 0.0268 | Î»: 0.900\nEpoch 80 | Loss: 0.0655 | Intra: 0.4563 | Inter: 0.0220 | Î»: 0.9000\nUpdating embeddings for hard negative mining...\nâœ“ Embeddings updated (epoch 80)\n\n======================================================================\nEvaluating Embedding Quality\n======================================================================\nAverage Intra-class Distance: 0.4957\nAverage Inter-class Distance: 1.1355\nSeparation Ratio (Inter/Intra): 2.2908\nâš ï¸  Warning: Separation ratio is low. Consider training longer.\n\n======================================================================\nPHASE 2: Training Prediction Network\n======================================================================\nEpoch   1 | LR: 0.00426 | Train Loss: 0.9969 | Train Acc: 80.59% | Test Acc: 82.38% âœ“ NEW BEST!\nEpoch   2 | LR: 0.00426 | Train Loss: 0.9301 | Train Acc: 82.34% | Test Acc: 82.80% âœ“ NEW BEST!\nEpoch   3 | LR: 0.00426 | Train Loss: 0.9240 | Train Acc: 82.52% | Test Acc: 82.82% âœ“ NEW BEST!\nEpoch   4 | LR: 0.00426 | Train Loss: 0.9190 | Train Acc: 82.42% | Test Acc: 82.70%\nEpoch   5 | LR: 0.00426 | Train Loss: 0.9182 | Train Acc: 82.30% | Test Acc: 82.44%\nEpoch   6 | LR: 0.00426 | Train Loss: 0.9145 | Train Acc: 82.62% | Test Acc: 82.56%\nEpoch   7 | LR: 0.00426 | Train Loss: 0.9139 | Train Acc: 82.43% | Test Acc: 82.79%\nEpoch   8 | LR: 0.00426 | Train Loss: 0.9130 | Train Acc: 82.28% | Test Acc: 82.74%\nEpoch   9 | LR: 0.00426 | Train Loss: 0.9130 | Train Acc: 82.36% | Test Acc: 82.82%\nEpoch  10 | LR: 0.00426 | Train Loss: 0.9103 | Train Acc: 82.45% | Test Acc: 82.85% âœ“ NEW BEST!\nEpoch  11 | LR: 0.00426 | Train Loss: 0.9105 | Train Acc: 82.43% | Test Acc: 82.74%\nEpoch  12 | LR: 0.00426 | Train Loss: 0.9127 | Train Acc: 82.38% | Test Acc: 82.72%\nEpoch  13 | LR: 0.00426 | Train Loss: 0.9103 | Train Acc: 82.33% | Test Acc: 82.76%\nEpoch  14 | LR: 0.00426 | Train Loss: 0.9085 | Train Acc: 82.43% | Test Acc: 82.90% âœ“ NEW BEST!\nEpoch  15 | LR: 0.00426 | Train Loss: 0.9092 | Train Acc: 82.44% | Test Acc: 82.88%\nEpoch  16 | LR: 0.00426 | Train Loss: 0.9065 | Train Acc: 82.58% | Test Acc: 82.70%\nEpoch  17 | LR: 0.00426 | Train Loss: 0.9077 | Train Acc: 82.40% | Test Acc: 82.91% âœ“ NEW BEST!\nEpoch  18 | LR: 0.00426 | Train Loss: 0.9051 | Train Acc: 82.59% | Test Acc: 82.95% âœ“ NEW BEST!\nEpoch  19 | LR: 0.00426 | Train Loss: 0.9074 | Train Acc: 82.45% | Test Acc: 82.85%\nEpoch  20 | LR: 0.00426 | Train Loss: 0.9057 | Train Acc: 82.56% | Test Acc: 82.89%\nEpoch  21 | LR: 0.00426 | Train Loss: 0.9070 | Train Acc: 82.48% | Test Acc: 82.88%\nEpoch  22 | LR: 0.00426 | Train Loss: 0.9036 | Train Acc: 82.61% | Test Acc: 82.83%\nEpoch  23 | LR: 0.00426 | Train Loss: 0.9038 | Train Acc: 82.50% | Test Acc: 82.85%\nEpoch  24 | LR: 0.00426 | Train Loss: 0.9036 | Train Acc: 82.69% | Test Acc: 83.05% âœ“ NEW BEST!\nEpoch  25 | LR: 0.00426 | Train Loss: 0.9059 | Train Acc: 82.41% | Test Acc: 82.79%\nEpoch  26 | LR: 0.00426 | Train Loss: 0.9037 | Train Acc: 82.57% | Test Acc: 83.02%\nEpoch  27 | LR: 0.00426 | Train Loss: 0.9023 | Train Acc: 82.64% | Test Acc: 82.86%\nEpoch  28 | LR: 0.00426 | Train Loss: 0.9051 | Train Acc: 82.42% | Test Acc: 82.92%\nEpoch  29 | LR: 0.00426 | Train Loss: 0.9028 | Train Acc: 82.57% | Test Acc: 82.90%\nEpoch  30 | LR: 0.00426 | Train Loss: 0.9045 | Train Acc: 82.46% | Test Acc: 82.85%\nEpoch  31 | LR: 0.00426 | Train Loss: 0.9072 | Train Acc: 82.37% | Test Acc: 82.86%\nEpoch  32 | LR: 0.00426 | Train Loss: 0.9057 | Train Acc: 82.45% | Test Acc: 82.87%\nEpoch  33 | LR: 0.00426 | Train Loss: 0.9000 | Train Acc: 82.67% | Test Acc: 82.96%\nEpoch  34 | LR: 0.00426 | Train Loss: 0.9030 | Train Acc: 82.59% | Test Acc: 82.92%\nEpoch  35 | LR: 0.00426 | Train Loss: 0.9007 | Train Acc: 82.81% | Test Acc: 82.97%\nEpoch  36 | LR: 0.00426 | Train Loss: 0.9038 | Train Acc: 82.61% | Test Acc: 82.95%\nEpoch  37 | LR: 0.00426 | Train Loss: 0.9034 | Train Acc: 82.57% | Test Acc: 83.00%\nEpoch  38 | LR: 0.00426 | Train Loss: 0.9018 | Train Acc: 82.53% | Test Acc: 82.81%\nEpoch  39 | LR: 0.00426 | Train Loss: 0.9032 | Train Acc: 82.62% | Test Acc: 82.97%\nEpoch  40 | LR: 0.00426 | Train Loss: 0.9030 | Train Acc: 82.63% | Test Acc: 83.06% âœ“ NEW BEST!\nEpoch  41 | LR: 0.00426 | Train Loss: 0.8994 | Train Acc: 82.72% | Test Acc: 82.91%\nEpoch  42 | LR: 0.00426 | Train Loss: 0.9045 | Train Acc: 82.46% | Test Acc: 82.84%\nEpoch  43 | LR: 0.00426 | Train Loss: 0.9009 | Train Acc: 82.64% | Test Acc: 83.14% âœ“ NEW BEST!\nEpoch  44 | LR: 0.00426 | Train Loss: 0.9008 | Train Acc: 82.76% | Test Acc: 82.78%\nEpoch  45 | LR: 0.00426 | Train Loss: 0.9006 | Train Acc: 82.65% | Test Acc: 83.04%\nEpoch  46 | LR: 0.00426 | Train Loss: 0.9007 | Train Acc: 82.78% | Test Acc: 83.13%\nEpoch  47 | LR: 0.00426 | Train Loss: 0.9009 | Train Acc: 82.73% | Test Acc: 83.09%\nEpoch  48 | LR: 0.00426 | Train Loss: 0.9023 | Train Acc: 82.79% | Test Acc: 82.89%\nEpoch  49 | LR: 0.00426 | Train Loss: 0.9015 | Train Acc: 82.56% | Test Acc: 82.83%\nEpoch  50 | LR: 0.00426 | Train Loss: 0.8977 | Train Acc: 82.73% | Test Acc: 82.96%\nEpoch  51 | LR: 0.00426 | Train Loss: 0.9011 | Train Acc: 82.75% | Test Acc: 82.98%\nEpoch  52 | LR: 0.00426 | Train Loss: 0.9018 | Train Acc: 82.57% | Test Acc: 82.80%\nEpoch  53 | LR: 0.00426 | Train Loss: 0.9001 | Train Acc: 82.50% | Test Acc: 83.02%\nEpoch  54 | LR: 0.00426 | Train Loss: 0.8997 | Train Acc: 82.82% | Test Acc: 82.88%\nEpoch  55 | LR: 0.00426 | Train Loss: 0.9031 | Train Acc: 82.56% | Test Acc: 82.88%\nEpoch  56 | LR: 0.00426 | Train Loss: 0.8985 | Train Acc: 82.76% | Test Acc: 82.81%\nEpoch  57 | LR: 0.00426 | Train Loss: 0.8999 | Train Acc: 82.73% | Test Acc: 82.92%\nEpoch  58 | LR: 0.00426 | Train Loss: 0.8996 | Train Acc: 82.79% | Test Acc: 82.76%\nEpoch  59 | LR: 0.00426 | Train Loss: 0.9017 | Train Acc: 82.65% | Test Acc: 82.96%\nEpoch  60 | LR: 0.00426 | Train Loss: 0.8980 | Train Acc: 82.75% | Test Acc: 82.93%\nEpoch  61 | LR: 0.00426 | Train Loss: 0.9012 | Train Acc: 82.49% | Test Acc: 82.95%\nEpoch  62 | LR: 0.00426 | Train Loss: 0.8978 | Train Acc: 82.67% | Test Acc: 83.01%\nEpoch  63 | LR: 0.00426 | Train Loss: 0.8979 | Train Acc: 82.62% | Test Acc: 82.91%\n\nEarly stopping triggered at epoch 63 (patience=20)\n\n======================================================================\nFINAL RESULTS\n======================================================================\nBest Test Accuracy: 83.14% (achieved at epoch 43)\nEmbedding Separation Ratio: 2.29x\n\nâœ“ SUCCESS! Target achieved (83.14% >= 80%)\n\n======================================================================\nTraining completed! Best model saved to 'best_optimized_model.pth'\n======================================================================\n","output_type":"stream"}],"execution_count":1}]}