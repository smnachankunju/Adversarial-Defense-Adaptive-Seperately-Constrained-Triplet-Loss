{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# cifar10_asctl.py\n# A-SCTL on CIFAR-10 with architecture and training flow matching MNIST implementation\n\nimport os\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Sampler, Subset, random_split\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n\n# ==================== Configuration ====================\nclass Config:\n    data_dir = \"./data\"\n    out_dir = \"./output_cifar10\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Network parameters\n    embedding_dim = 128\n    \n    # PK Sampling\n    batch_p = 10      # P classes per batch\n    batch_k = 6       # K samples per class\n    \n    # A-SCTL parameters (same as MNIST / paper)\n    margin_intra = 0.01   # m1\n    margin_inter = 1.0    # m2\n    beta_min = 0.1\n    beta_max = 0.9\n    eps = 1e-8\n    \n    # Training parameters\n    lr = 1e-3\n    momentum = 0.9\n    weight_decay = 1e-5\n    epochs = 20\n    seed = 42\n    use_lr_scheduler = True  # cosine annealing, same as MNIST\n    \n    # Prediction network training\n    pred_lr = 1e-3\n    pred_momentum = 0.9\n    pred_epochs = 10\n    pred_batch_size = 256\n    \n    # Dataset split (80/10/10 as per paper)\n    train_split = 0.8\n    test_split = 0.1\n    val_split = 0.1\n    \n    # Monitoring and debugging\n    num_workers = 2\n    pin_memory = True\n    print_every = 50\n    save_embeddings = True\n    embeddings_file = \"cifar10_train_embeddings.npy\"\n    labels_file = \"cifar10_train_labels.npy\"\n    debug_triplets = True\n    log_beta_history = True\n    beta_history_file = \"cifar10_beta_evolution.npy\"\n    validate_every = 5  # Validate every N epochs\n    monitor_collapse = True  # Check for embedding collapse\n\n\n# ==================== PK Batch Sampler ====================\nclass PKSampler(Sampler):\n    \"\"\"Samples P classes, then K samples per class.\"\"\"\n    def __init__(self, labels, P, K):\n        self.labels = np.array(labels)\n        self.P = P\n        self.K = K\n        \n        # Group indices by label\n        self.label_to_indices = defaultdict(list)\n        for idx, label in enumerate(labels):\n            self.label_to_indices[label].append(idx)\n        \n        # Verify each class has at least K samples\n        for label, indices in self.label_to_indices.items():\n            if len(indices) < self.K:\n                raise ValueError(f\"Label {label} has only {len(indices)} samples, need {self.K}\")\n        \n        self.labels_set = list(self.label_to_indices.keys())\n        self.num_samples = len(labels)\n        \n        # Calculate batches per epoch\n        self.batches_per_epoch = max(1, self.num_samples // (self.P * self.K))\n        \n        print(f\"PKSampler initialized: {len(self.labels_set)} classes, \"\n              f\"{self.batches_per_epoch} batches/epoch, \"\n              f\"batch_size={self.P*self.K}\")\n    \n    def __iter__(self):\n        for _ in range(self.batches_per_epoch):\n            # Randomly select P classes\n            if self.P > len(self.labels_set):\n                raise ValueError(f\"P={self.P} exceeds number of classes {len(self.labels_set)}\")\n            \n            selected_classes = np.random.choice(self.labels_set, self.P, replace=False)\n            batch_indices = []\n            \n            # Sample K instances from each selected class\n            for cls in selected_classes:\n                indices = np.random.choice(self.label_to_indices[cls], self.K, replace=False)\n                batch_indices.extend(indices.tolist())\n            \n            yield batch_indices\n    \n    def __len__(self):\n        return self.batches_per_epoch\n\n\n# ==================== CNN Encoder (Paper Architecture, 3-channel CIFAR) ====================\nclass EncoderCNN(nn.Module):\n    \"\"\"\n    CNN architecture from paper Section IV-B:\n    Input: 3 channels (CIFAR-10)\n    Conv layers: (3, 5x5) -> (64, 5x5) -> (128, 3x3) -> (256, 3x3) -> (128, 2x2)\n    Output: 128-dimensional L2-normalized embeddings\n    \"\"\"\n    def __init__(self, embedding_dim=128, input_channels=3):\n        super().__init__()\n        \n        # Convolutional layers as specified in paper\n        self.features = nn.Sequential(\n            # Layer 1: (3, 5x5) -> 64 channels\n            nn.Conv2d(input_channels, 64, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 32x32 -> 16x16\n            \n            # Layer 2: (64, 5x5) -> 128 channels\n            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),  # 16x16 -> 8x8\n            \n            # Layer 3: (128, 3x3) -> 256 channels\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            \n            # Layer 4: (256, 3x3) -> 128 channels\n            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            \n            # Layer 5: (128, 2x2) -> 128 channels\n            nn.Conv2d(128, 128, kernel_size=2, padding=0),\n            nn.ReLU(inplace=True),\n            \n            # Global average pooling\n            nn.AdaptiveAvgPool2d(1)\n        )\n        \n        # Fully connected to embedding dimension\n        self.fc = nn.Linear(128, embedding_dim)\n        self.bn = nn.BatchNorm1d(embedding_dim)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = self.bn(x)\n        # L2 normalization - ensures embeddings have unit norm\n        x = F.normalize(x, p=2, dim=1)\n        return x\n\n\n# ==================== Prediction Network ====================\nclass PredictionNetwork(nn.Module):\n    \"\"\"\n    Simple 1-layer MLP for classification from embeddings.\n    Same as MNIST implementation: 128-dim -> hidden -> 10-dim.\n    \"\"\"\n    def __init__(self, embedding_dim=128, num_classes=10, hidden_dim=256):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(embedding_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\n\n# ==================== Enhanced Triplet Mining ====================\ndef mine_hard_triplets_batch(embeddings, labels, debug=False, return_stats=False):\n    \"\"\"\n    Same as MNIST implementation.\n    Enhanced online hard triplet mining with quality metrics.\n    \"\"\"\n    embeddings = embeddings.detach()\n    labels_np = labels.cpu().numpy()\n    N = embeddings.size(0)\n    \n    # Validate batch composition\n    unique_classes = np.unique(labels_np)\n    if len(unique_classes) < 2:\n        if debug:\n            print(f\"⚠️  WARNING: Only {len(unique_classes)} class(es) in batch!\")\n        return [] if not return_stats else ([], {})\n    \n    # Compute pairwise distances\n    dists = torch.cdist(embeddings, embeddings, p=2)\n    \n    # Initialize statistics\n    stats = {\n        'avg_d_ap': 0.0,\n        'avg_d_an': 0.0,\n        'margin_violations': 0.0,\n        'semi_hard_ratio': 0.0,\n        'min_dist': 0.0,\n        'max_dist': 0.0,\n        'mean_dist': 0.0\n    }\n    \n    # Debug: Check distance statistics\n    if debug or return_stats:\n        max_dist = dists.max().item()\n        min_dist = dists[dists > 0].min().item() if (dists > 0).any() else 0\n        mean_dist = dists[dists > 0].mean().item() if (dists > 0).any() else 0\n        stats['min_dist'] = min_dist\n        stats['max_dist'] = max_dist\n        stats['mean_dist'] = mean_dist\n        \n        if debug:\n            print(f\"  Distance range: [{min_dist:.4f}, {max_dist:.4f}], mean: {mean_dist:.4f}\")\n            norms = torch.norm(embeddings, p=2, dim=1)\n            print(f\"  Embedding norms: mean={norms.mean():.4f}, std={norms.std():.4f}\")\n    \n    triplets = []\n    no_pos_count = 0\n    no_neg_count = 0\n    semi_hard_count = 0\n    d_ap_sum = 0.0\n    d_an_sum = 0.0\n    violation_count = 0\n    \n    for i in range(N):\n        anchor_label = labels_np[i]\n        \n        pos_mask = (labels_np == anchor_label)\n        pos_mask[i] = False\n        pos_indices = np.where(pos_mask)[0]\n        \n        if len(pos_indices) == 0:\n            no_pos_count += 1\n            continue\n        \n        neg_indices = np.where(labels_np != anchor_label)[0]\n        if len(neg_indices) == 0:\n            no_neg_count += 1\n            continue\n        \n        pos_dists = dists[i, pos_indices]\n        hardest_pos_idx = pos_indices[torch.argmax(pos_dists).item()]\n        d_ap = dists[i, hardest_pos_idx].item()\n        \n        neg_dists = dists[i, neg_indices]\n        \n        semi_hard_mask = neg_dists > d_ap\n        if semi_hard_mask.any():\n            semi_hard_indices = neg_indices[semi_hard_mask.cpu().numpy()]\n            semi_hard_dists = neg_dists[semi_hard_mask]\n            hardest_neg_idx = semi_hard_indices[torch.argmin(semi_hard_dists).item()]\n            semi_hard_count += 1\n        else:\n            hardest_neg_idx = neg_indices[torch.argmin(neg_dists).item()]\n        \n        d_an = dists[i, hardest_neg_idx].item()\n        \n        d_ap_sum += d_ap\n        d_an_sum += d_an\n        if d_ap >= d_an:\n            violation_count += 1\n        \n        triplets.append((i, hardest_pos_idx, int(hardest_neg_idx)))\n    \n    if debug and (no_pos_count > 0 or no_neg_count > 0):\n        print(f\"  Skipped: {no_pos_count} (no pos), {no_neg_count} (no neg)\")\n    \n    if len(triplets) == 0:\n        if debug:\n            print(f\"⚠️  WARNING: No valid triplets found! Batch size={N}, Classes={len(unique_classes)}\")\n        return [] if not return_stats else ([], stats)\n    \n    if return_stats:\n        stats['avg_d_ap'] = d_ap_sum / len(triplets)\n        stats['avg_d_an'] = d_an_sum / len(triplets)\n        stats['margin_violations'] = violation_count / len(triplets)\n        stats['semi_hard_ratio'] = semi_hard_count / len(triplets)\n    \n    return triplets if not return_stats else (triplets, stats)\n\n\n# ==================== Embedding Collapse Monitor ====================\ndef check_embedding_collapse(embeddings, threshold=0.1):\n    with torch.no_grad():\n        pairwise_dists = torch.cdist(embeddings, embeddings, p=2)\n        mask = ~torch.eye(pairwise_dists.size(0), dtype=torch.bool, device=pairwise_dists.device)\n        valid_dists = pairwise_dists[mask]\n        \n        mean_dist = valid_dists.mean().item()\n        std_dist = valid_dists.std().item()\n        is_collapsing = mean_dist < threshold\n        \n    return is_collapsing, mean_dist, std_dist\n\n\n# ==================== Validation Function ====================\ndef validate_embedding(model, val_dataset, cfg):\n    model.eval()\n    \n    val_labels = [int(val_dataset.dataset[idx][1]) for idx in val_dataset.indices]\n    pk_sampler = PKSampler(val_labels, cfg.batch_p, cfg.batch_k)\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_sampler=pk_sampler,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory\n    )\n    \n    total_loss = 0.0\n    total_lintra = 0.0\n    total_linter = 0.0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(cfg.device)\n            labels = labels.to(cfg.device)\n            \n            embeddings = model(images)\n            triplets = mine_hard_triplets_batch(embeddings, labels, debug=False)\n            \n            if len(triplets) == 0:\n                continue\n            \n            d_ap_list = []\n            d_an_list = []\n            \n            for (anchor_idx, pos_idx, neg_idx) in triplets:\n                d_ap = torch.norm(embeddings[anchor_idx] - embeddings[pos_idx], p=2)\n                d_an = torch.norm(embeddings[anchor_idx] - embeddings[neg_idx], p=2)\n                d_ap_list.append(d_ap)\n                d_an_list.append(d_an)\n            \n            d_ap = torch.stack(d_ap_list)\n            d_an = torch.stack(d_an_list)\n            \n            L_intra = F.relu(d_ap - cfg.margin_intra).mean()\n            L_inter = F.relu(cfg.margin_inter - d_an).mean()\n            \n            beta = L_intra / (L_intra + L_inter + cfg.eps)\n            beta = torch.clamp(beta, cfg.beta_min, cfg.beta_max).item()\n            \n            loss = (1.0 - beta) * L_intra + beta * L_inter\n            \n            total_loss += loss.item()\n            total_lintra += L_intra.item()\n            total_linter += L_inter.item()\n            num_batches += 1\n    \n    if num_batches == 0:\n        return 0.0, 0.0, 0.0\n    \n    avg_loss = total_loss / num_batches\n    avg_lintra = total_lintra / num_batches\n    avg_linter = total_linter / num_batches\n    \n    return avg_loss, avg_lintra, avg_linter\n\n\n# ==================== Enhanced Training with Monitoring ====================\ndef train_epoch_embedding(model, loader, optimizer, scheduler, cfg, epoch, val_dataset=None):\n    model.train()\n    \n    epoch_loss = 0.0\n    epoch_lintra = 0.0\n    epoch_linter = 0.0\n    epoch_betas = []\n    epoch_stats = defaultdict(list)\n    num_batches = 0\n    total_triplets = 0\n    \n    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{cfg.epochs}\", leave=False)\n    \n    for batch_idx, (images, labels) in enumerate(pbar):\n        images = images.to(cfg.device)\n        labels = labels.to(cfg.device)\n        \n        optimizer.zero_grad()\n        embeddings = model(images)\n        \n        if cfg.monitor_collapse:\n            is_collapsing, mean_dist, std_dist = check_embedding_collapse(embeddings)\n            if is_collapsing and batch_idx % 100 == 0:\n                print(f\"\\n⚠️  WARNING: Potential embedding collapse! Mean dist: {mean_dist:.4f}\")\n        \n        debug_mode = (cfg.debug_triplets and epoch == 1 and batch_idx == 0)\n        \n        return_stats = (batch_idx == 0)\n        result = mine_hard_triplets_batch(embeddings, labels, debug=debug_mode, return_stats=return_stats)\n        \n        if return_stats:\n            triplets, stats = result\n            for key, val in stats.items():\n                epoch_stats[key].append(val)\n        else:\n            triplets = result\n        \n        if len(triplets) == 0:\n            continue\n        \n        d_ap_list = []\n        d_an_list = []\n        \n        for (anchor_idx, pos_idx, neg_idx) in triplets:\n            d_ap = torch.norm(embeddings[anchor_idx] - embeddings[pos_idx], p=2)\n            d_an = torch.norm(embeddings[anchor_idx] - embeddings[neg_idx], p=2)\n            d_ap_list.append(d_ap)\n            d_an_list.append(d_an)\n        \n        d_ap = torch.stack(d_ap_list)\n        d_an = torch.stack(d_an_list)\n        \n        L_intra_terms = F.relu(d_ap - cfg.margin_intra)\n        L_inter_terms = F.relu(cfg.margin_inter - d_an)\n        \n        L_intra = L_intra_terms.mean()\n        L_inter = L_inter_terms.mean()\n        \n        with torch.no_grad():\n            beta = L_intra / (L_intra + L_inter + cfg.eps)\n            beta = torch.clamp(beta, cfg.beta_min, cfg.beta_max).item()\n        \n        loss = (1.0 - beta) * L_intra + beta * L_inter\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_lintra += L_intra.item()\n        epoch_linter += L_inter.item()\n        epoch_betas.append(beta)\n        num_batches += 1\n        total_triplets += len(triplets)\n        \n        if (batch_idx + 1) % cfg.print_every == 0 or batch_idx == 0:\n            avg_loss = epoch_loss / num_batches\n            avg_lintra = epoch_lintra / num_batches\n            avg_linter = epoch_linter / num_batches\n            avg_beta = np.mean(epoch_betas)\n            \n            pbar.set_postfix({\n                'loss': f'{avg_loss:.4f}',\n                'L_in': f'{avg_lintra:.4f}',\n                'L_out': f'{avg_linter:.4f}',\n                'β': f'{avg_beta:.3f}',\n                'trip': total_triplets\n            })\n    \n    if scheduler is not None:\n        scheduler.step()\n    \n    val_info = \"\"\n    if val_dataset is not None and epoch % cfg.validate_every == 0:\n        val_loss, val_lintra, val_linter = validate_embedding(model, val_dataset, cfg)\n        val_info = f\" | Val Loss: {val_loss:.4f}\"\n    \n    avg_loss = epoch_loss / max(num_batches, 1)\n    avg_lintra = epoch_lintra / max(num_batches, 1)\n    avg_linter = epoch_linter / max(num_batches, 1)\n    avg_beta = np.mean(epoch_betas) if len(epoch_betas) > 0 else 0.5\n    \n    if epoch_stats:\n        stats_str = \" | \".join([f\"{k}: {np.mean(v):.4f}\" for k, v in epoch_stats.items() if v])\n        if stats_str:\n            print(f\"  Triplet stats: {stats_str}\")\n    \n    return avg_loss, avg_lintra, avg_linter, avg_beta, val_info\n\n\ndef train_prediction_network(encoder, pred_net, train_dataset, test_dataset, cfg):\n    print(\"\\n\" + \"=\"*60)\n    print(\"Training Prediction Network (CIFAR-10)\")\n    print(\"=\"*60)\n    \n    encoder.eval()\n    pred_net.train()\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=cfg.pred_batch_size,\n        shuffle=True,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=cfg.pred_batch_size,\n        shuffle=False,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory\n    )\n    \n    optimizer = torch.optim.SGD(\n        pred_net.parameters(),\n        lr=cfg.pred_lr,\n        momentum=cfg.pred_momentum,\n        weight_decay=cfg.weight_decay\n    )\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.pred_epochs)\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    best_acc = 0.0\n    \n    for epoch in range(1, cfg.pred_epochs + 1):\n        pred_net.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n        \n        for images, labels in tqdm(train_loader, desc=f\"Pred Epoch {epoch}/{cfg.pred_epochs}\", leave=False):\n            images = images.to(cfg.device)\n            labels = labels.to(cfg.device)\n            \n            with torch.no_grad():\n                embeddings = encoder(images)\n            \n            optimizer.zero_grad()\n            outputs = pred_net(embeddings)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100. * correct / total\n        \n        scheduler.step()\n        \n        test_acc = evaluate_classification(encoder, pred_net, test_loader, cfg)\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Pred Epoch {epoch}/{cfg.pred_epochs} | \"\n              f\"LR: {current_lr:.6f} | \"\n              f\"Train Loss: {train_loss/len(train_loader):.4f} | \"\n              f\"Train Acc: {train_acc:.2f}% | \"\n              f\"Test Acc: {test_acc:.2f}%\")\n        \n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save({\n                'encoder': encoder.state_dict(),\n                'predictor': pred_net.state_dict(),\n                'accuracy': best_acc\n            }, os.path.join(cfg.out_dir, 'best_cifar10_model.pt'))\n            print(f\"  ✓ New best model saved! Accuracy: {best_acc:.2f}%\")\n    \n    print(f\"\\nBest Test Accuracy (CIFAR-10): {best_acc:.2f}%\")\n    return best_acc\n\n\ndef evaluate_classification(encoder, pred_net, test_loader, cfg):\n    encoder.eval()\n    pred_net.eval()\n    \n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(cfg.device)\n            labels = labels.to(cfg.device)\n            \n            embeddings = encoder(images)\n            outputs = pred_net(embeddings)\n            \n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    accuracy = 100. * correct / total\n    return accuracy\n\n\ndef extract_embeddings(model, dataset, cfg):\n    model.eval()\n    loader = DataLoader(\n        dataset,\n        batch_size=256,\n        shuffle=False,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory\n    )\n    \n    all_embeddings = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(loader, desc=\"Extracting embeddings (CIFAR-10)\"):\n            images = images.to(cfg.device)\n            embeddings = model(images).cpu().numpy()\n            all_embeddings.append(embeddings)\n            all_labels.append(labels.numpy())\n    \n    embeddings = np.vstack(all_embeddings)\n    labels = np.concatenate(all_labels)\n    \n    os.makedirs(cfg.out_dir, exist_ok=True)\n    np.save(os.path.join(cfg.out_dir, cfg.embeddings_file), embeddings)\n    np.save(os.path.join(cfg.out_dir, cfg.labels_file), labels)\n    \n    print(f\"\\n✓ Saved CIFAR-10 embeddings: {embeddings.shape}\")\n    return embeddings, labels\n\n\n# ==================== Dataset Split (80/10/10) ====================\ndef create_paper_splits(full_dataset, train_ratio=0.8, test_ratio=0.1, val_ratio=0.1, seed=42):\n    assert abs(train_ratio + test_ratio + val_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n    \n    total_size = len(full_dataset)\n    train_size = int(train_ratio * total_size)\n    test_size = int(test_ratio * total_size)\n    val_size = total_size - train_size - test_size\n    \n    generator = torch.Generator().manual_seed(seed)\n    train_dataset, test_dataset, val_dataset = random_split(\n        full_dataset, [train_size, test_size, val_size], generator=generator\n    )\n    \n    return train_dataset, test_dataset, val_dataset\n\n\n# ==================== Main ====================\ndef main():\n    cfg = Config()\n    \n    os.makedirs(cfg.out_dir, exist_ok=True)\n    \n    torch.manual_seed(cfg.seed)\n    np.random.seed(cfg.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(cfg.seed)\n    \n    print(\"=\"*60)\n    print(\"A-SCTL Training on CIFAR-10 (Same Implementation as MNIST)\")\n    print(\"=\"*60)\n    print(f\"Device: {cfg.device}\")\n    print(f\"Embedding dim: {cfg.embedding_dim}\")\n    print(f\"Margins: m1={cfg.margin_intra}, m2={cfg.margin_inter}\")\n    print(f\"Beta range: [{cfg.beta_min}, {cfg.beta_max}]\")\n    print(f\"Batch sampling: P={cfg.batch_p}, K={cfg.batch_k}\")\n    print(f\"Optimizer: SGD (lr={cfg.lr}, momentum={cfg.momentum})\")\n    print(f\"LR Scheduler: {'Enabled (Cosine)' if cfg.use_lr_scheduler else 'Disabled'}\")\n    print(f\"Dataset split: {cfg.train_split}/{cfg.test_split}/{cfg.val_split}\")\n    print(f\"Monitoring: Collapse={cfg.monitor_collapse}, Validation every {cfg.validate_every} epochs\")\n    print(\"=\"*60 + \"\\n\")\n    \n    # CIFAR-10 transforms (standard)\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            (0.4914, 0.4822, 0.4465),\n            (0.2023, 0.1994, 0.2010)\n        )\n    ])\n    \n    transform_full = transform_train  # for splitting; test transform can be same or simpler\n    \n    full_dataset = datasets.CIFAR10(\n        cfg.data_dir, train=True, download=True, transform=transform_full\n    )\n    \n    train_dataset, test_dataset, val_dataset = create_paper_splits(\n        full_dataset, cfg.train_split, cfg.test_split, cfg.val_split, cfg.seed\n    )\n    \n    print(f\"Dataset sizes (CIFAR-10):\")\n    print(f\"  Training:   {len(train_dataset)} samples\")\n    print(f\"  Test:       {len(test_dataset)} samples\")\n    print(f\"  Validation: {len(val_dataset)} samples\")\n    print()\n    \n    train_labels = [int(train_dataset.dataset[idx][1]) for idx in train_dataset.indices]\n    \n    pk_sampler = PKSampler(train_labels, cfg.batch_p, cfg.batch_k)\n    print()\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_sampler=pk_sampler,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory\n    )\n    \n    encoder = EncoderCNN(cfg.embedding_dim, input_channels=3).to(cfg.device)\n    \n    optimizer = torch.optim.SGD(\n        encoder.parameters(),\n        lr=cfg.lr,\n        momentum=cfg.momentum,\n        weight_decay=cfg.weight_decay\n    )\n    \n    scheduler = None\n    if cfg.use_lr_scheduler:\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n    \n    print(\"Phase 1: Training Embedding Network with A-SCTL (CIFAR-10)\")\n    print(\"-\"*60)\n    print(\"Same features as MNIST: LR scheduling, validation, collapse monitoring\\n\")\n    \n    all_beta_history = []\n    \n    for epoch in range(1, cfg.epochs + 1):\n        loss, L_intra, L_inter, beta, val_info = train_epoch_embedding(\n            encoder, train_loader, optimizer, scheduler, cfg, epoch, \n            val_dataset=(val_dataset if epoch % cfg.validate_every == 0 else None)\n        )\n        \n        all_beta_history.append(beta)\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Epoch {epoch:2d}/{cfg.epochs} | \"\n              f\"LR: {current_lr:.6f} | \"\n              f\"Loss: {loss:.4f} | \"\n              f\"L_intra: {L_intra:.4f} | \"\n              f\"L_inter: {L_inter:.4f} | \"\n              f\"β_avg: {beta:.4f}{val_info}\")\n        \n        if epoch % 5 == 0 or epoch == cfg.epochs:\n            ckpt_path = os.path.join(cfg.out_dir, f'encoder_cifar10_epoch{epoch}.pt')\n            torch.save(encoder.state_dict(), ckpt_path)\n            \n            if cfg.log_beta_history:\n                beta_path = os.path.join(cfg.out_dir, f'cifar10_beta_epoch{epoch}.npy')\n                np.save(beta_path, np.array(all_beta_history))\n    \n    if cfg.log_beta_history:\n        final_beta_path = os.path.join(cfg.out_dir, cfg.beta_history_file)\n        np.save(final_beta_path, np.array(all_beta_history))\n        print(f\"\\n✓ Saved CIFAR-10 beta evolution history: {final_beta_path}\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Phase 2: Training Prediction Network (CIFAR-10)\")\n    print(\"-\"*60)\n    \n    pred_net = PredictionNetwork(cfg.embedding_dim, num_classes=10).to(cfg.device)\n    best_acc = train_prediction_network(encoder, pred_net, train_dataset, test_dataset, cfg)\n    \n    if cfg.save_embeddings:\n        print(\"\\n\" + \"=\"*60)\n        print(\"Extracting CIFAR-10 Embeddings\")\n        print(\"-\"*60)\n        extract_embeddings(encoder, train_dataset, cfg)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"CIFAR-10 Training Complete!\")\n    print(f\"Final Test Accuracy: {best_acc:.2f}%\")\n    print(\"=\"*60)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:18:10.293418Z","iopub.execute_input":"2025-12-14T20:18:10.294005Z","iopub.status.idle":"2025-12-14T20:37:39.539757Z","shell.execute_reply.started":"2025-12-14T20:18:10.293975Z","shell.execute_reply":"2025-12-14T20:37:39.538771Z"}},"outputs":[{"name":"stdout","text":"============================================================\nA-SCTL Training on CIFAR-10 (Same Implementation as MNIST)\n============================================================\nDevice: cuda\nEmbedding dim: 128\nMargins: m1=0.01, m2=1.0\nBeta range: [0.1, 0.9]\nBatch sampling: P=10, K=6\nOptimizer: SGD (lr=0.001, momentum=0.9)\nLR Scheduler: Enabled (Cosine)\nDataset split: 0.8/0.1/0.1\nMonitoring: Collapse=True, Validation every 5 epochs\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:01<00:00, 105MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Dataset sizes (CIFAR-10):\n  Training:   40000 samples\n  Test:       5000 samples\n  Validation: 5000 samples\n\nPKSampler initialized: 10 classes, 666 batches/epoch, batch_size=60\n\nPhase 1: Training Embedding Network with A-SCTL (CIFAR-10)\n------------------------------------------------------------\nSame features as MNIST: LR scheduling, validation, collapse monitoring\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20:   0%|          | 0/666 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"  Distance range: [0.0002, 1.9249], mean: 1.3799\n  Embedding norms: mean=1.0000, std=0.0000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.6531 | avg_d_an: 1.4921 | margin_violations: 0.1333 | semi_hard_ratio: 0.8667 | min_dist: 0.0002 | max_dist: 1.9249 | mean_dist: 1.3799\nEpoch  1/20 | LR: 0.000994 | Loss: 0.1507 | L_intra: 1.4805 | L_inter: 0.0029 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.5090 | avg_d_an: 1.5270 | margin_violations: 0.0000 | semi_hard_ratio: 1.0000 | min_dist: 0.0002 | max_dist: 1.8988 | mean_dist: 1.3914\nEpoch  2/20 | LR: 0.000976 | Loss: 0.1451 | L_intra: 1.4320 | L_inter: 0.0021 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.3814 | avg_d_an: 1.4047 | margin_violations: 0.0000 | semi_hard_ratio: 1.0000 | min_dist: 0.0002 | max_dist: 1.9165 | mean_dist: 1.3361\nEpoch  3/20 | LR: 0.000946 | Loss: 0.1427 | L_intra: 1.4024 | L_inter: 0.0028 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.4439 | avg_d_an: 1.4399 | margin_violations: 0.0333 | semi_hard_ratio: 0.9667 | min_dist: 0.0002 | max_dist: 1.8848 | mean_dist: 1.3585\nEpoch  4/20 | LR: 0.000905 | Loss: 0.1414 | L_intra: 1.3867 | L_inter: 0.0030 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"PKSampler initialized: 10 classes, 83 batches/epoch, batch_size=60\n  Triplet stats: avg_d_ap: 1.3638 | avg_d_an: 1.3699 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.8614 | mean_dist: 1.3645\nEpoch  5/20 | LR: 0.000854 | Loss: 0.1399 | L_intra: 1.3730 | L_inter: 0.0028 | β_avg: 0.9000 | Val Loss: 0.1384\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.4923 | avg_d_an: 1.4900 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.8865 | mean_dist: 1.3519\nEpoch  6/20 | LR: 0.000794 | Loss: 0.1389 | L_intra: 1.3596 | L_inter: 0.0033 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.3873 | avg_d_an: 1.4061 | margin_violations: 0.0000 | semi_hard_ratio: 1.0000 | min_dist: 0.0002 | max_dist: 1.8967 | mean_dist: 1.3703\nEpoch  7/20 | LR: 0.000727 | Loss: 0.1379 | L_intra: 1.3489 | L_inter: 0.0033 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.4020 | avg_d_an: 1.4068 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.9281 | mean_dist: 1.3470\nEpoch  8/20 | LR: 0.000655 | Loss: 0.1368 | L_intra: 1.3408 | L_inter: 0.0030 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.3990 | avg_d_an: 1.4029 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.8635 | mean_dist: 1.3748\nEpoch  9/20 | LR: 0.000578 | Loss: 0.1358 | L_intra: 1.3298 | L_inter: 0.0032 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"PKSampler initialized: 10 classes, 83 batches/epoch, batch_size=60\n  Triplet stats: avg_d_ap: 1.4091 | avg_d_an: 1.4228 | margin_violations: 0.0000 | semi_hard_ratio: 1.0000 | min_dist: 0.0002 | max_dist: 1.8326 | mean_dist: 1.3800\nEpoch 10/20 | LR: 0.000500 | Loss: 0.1349 | L_intra: 1.3214 | L_inter: 0.0031 | β_avg: 0.9000 | Val Loss: 0.1312\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.3117 | avg_d_an: 1.3158 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.8550 | mean_dist: 1.3044\nEpoch 11/20 | LR: 0.000422 | Loss: 0.1339 | L_intra: 1.3102 | L_inter: 0.0033 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.3972 | avg_d_an: 1.4067 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.9015 | mean_dist: 1.3520\nEpoch 12/20 | LR: 0.000345 | Loss: 0.1320 | L_intra: 1.2920 | L_inter: 0.0031 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.3097 | avg_d_an: 1.3136 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.8493 | mean_dist: 1.2862\nEpoch 13/20 | LR: 0.000273 | Loss: 0.1204 | L_intra: 1.1526 | L_inter: 0.0057 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.0934 | avg_d_an: 1.0836 | margin_violations: 0.0333 | semi_hard_ratio: 0.9667 | min_dist: 0.0002 | max_dist: 1.5638 | mean_dist: 1.0719\nEpoch 14/20 | LR: 0.000206 | Loss: 0.1164 | L_intra: 1.1040 | L_inter: 0.0066 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"PKSampler initialized: 10 classes, 83 batches/epoch, batch_size=60\n  Triplet stats: avg_d_ap: 1.1193 | avg_d_an: 1.1288 | margin_violations: 0.0000 | semi_hard_ratio: 1.0000 | min_dist: 0.0002 | max_dist: 1.5311 | mean_dist: 1.0932\nEpoch 15/20 | LR: 0.000146 | Loss: 0.1149 | L_intra: 1.0948 | L_inter: 0.0060 | β_avg: 0.9000 | Val Loss: 0.1294\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.1194 | avg_d_an: 1.1322 | margin_violations: 0.0000 | semi_hard_ratio: 1.0000 | min_dist: 0.0002 | max_dist: 1.5194 | mean_dist: 1.0850\nEpoch 16/20 | LR: 0.000095 | Loss: 0.1141 | L_intra: 1.0907 | L_inter: 0.0056 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.1112 | avg_d_an: 1.1140 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.4903 | mean_dist: 1.0633\nEpoch 17/20 | LR: 0.000054 | Loss: 0.1138 | L_intra: 1.0869 | L_inter: 0.0057 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.1048 | avg_d_an: 1.1057 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.4441 | mean_dist: 1.0792\nEpoch 18/20 | LR: 0.000024 | Loss: 0.1133 | L_intra: 1.0840 | L_inter: 0.0054 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"  Triplet stats: avg_d_ap: 1.0854 | avg_d_an: 1.0927 | margin_violations: 0.0000 | semi_hard_ratio: 1.0000 | min_dist: 0.0003 | max_dist: 1.4445 | mean_dist: 1.0618\nEpoch 19/20 | LR: 0.000006 | Loss: 0.1131 | L_intra: 1.0823 | L_inter: 0.0054 | β_avg: 0.9000\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"PKSampler initialized: 10 classes, 83 batches/epoch, batch_size=60\n  Triplet stats: avg_d_ap: 1.0889 | avg_d_an: 1.0917 | margin_violations: 0.0167 | semi_hard_ratio: 0.9833 | min_dist: 0.0002 | max_dist: 1.4485 | mean_dist: 1.0712\nEpoch 20/20 | LR: 0.000000 | Loss: 0.1128 | L_intra: 1.0810 | L_inter: 0.0052 | β_avg: 0.9000 | Val Loss: 0.1128\n\n✓ Saved CIFAR-10 beta evolution history: ./output_cifar10/cifar10_beta_evolution.npy\n\n============================================================\nPhase 2: Training Prediction Network (CIFAR-10)\n------------------------------------------------------------\n\n============================================================\nTraining Prediction Network (CIFAR-10)\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 1/10 | LR: 0.000976 | Train Loss: 2.2988 | Train Acc: 11.23% | Test Acc: 11.94%\n  ✓ New best model saved! Accuracy: 11.94%\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 2/10 | LR: 0.000905 | Train Loss: 2.2901 | Train Acc: 14.72% | Test Acc: 17.76%\n  ✓ New best model saved! Accuracy: 17.76%\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 3/10 | LR: 0.000794 | Train Loss: 2.2820 | Train Acc: 19.39% | Test Acc: 25.38%\n  ✓ New best model saved! Accuracy: 25.38%\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 4/10 | LR: 0.000655 | Train Loss: 2.2741 | Train Acc: 22.97% | Test Acc: 31.36%\n  ✓ New best model saved! Accuracy: 31.36%\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 5/10 | LR: 0.000500 | Train Loss: 2.2671 | Train Acc: 26.40% | Test Acc: 35.48%\n  ✓ New best model saved! Accuracy: 35.48%\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 6/10 | LR: 0.000345 | Train Loss: 2.2607 | Train Acc: 28.68% | Test Acc: 38.30%\n  ✓ New best model saved! Accuracy: 38.30%\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 7/10 | LR: 0.000206 | Train Loss: 2.2558 | Train Acc: 29.65% | Test Acc: 39.36%\n  ✓ New best model saved! Accuracy: 39.36%\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 8/10 | LR: 0.000095 | Train Loss: 2.2528 | Train Acc: 30.20% | Test Acc: 38.70%\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 9/10 | LR: 0.000024 | Train Loss: 2.2507 | Train Acc: 30.92% | Test Acc: 40.56%\n  ✓ New best model saved! Accuracy: 40.56%\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Pred Epoch 10/10 | LR: 0.000000 | Train Loss: 2.2500 | Train Acc: 30.66% | Test Acc: 39.40%\n\nBest Test Accuracy (CIFAR-10): 40.56%\n\n============================================================\nExtracting CIFAR-10 Embeddings\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Extracting embeddings (CIFAR-10): 100%|██████████| 157/157 [00:09<00:00, 16.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n✓ Saved CIFAR-10 embeddings: (40000, 128)\n\n============================================================\nCIFAR-10 Training Complete!\nFinal Test Accuracy: 40.56%\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}